/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */

export interface paths {
  "/Experimental": {
    /**
     * Experimental
     * @description Experimental node.
     */
    post: operations["Experimental"];
  };
  "/Box": {
    /**
     * Box
     * @description Combine multiple values into a single output.
     */
    post: operations["Box"];
  };
  "/If": {
    /**
     * If
     * @description Return one of two options based on a condition.
     */
    post: operations["If"];
  };
  "/RunPython": {
    /**
     * RunPython
     * @description Run code using a Python interpreter.
     */
    post: operations["RunPython"];
  };
  "/ComputeText": {
    /**
     * ComputeText
     * @description Compute text using a language model.
     */
    post: operations["ComputeText"];
  };
  "/MultiComputeText": {
    /**
     * MultiComputeText
     * @description Generate multiple text choices using a language model.
     */
    post: operations["MultiComputeText"];
  };
  "/BatchComputeText": {
    /**
     * BatchComputeText
     * @description Compute text for multiple prompts in batch using a language model.
     */
    post: operations["BatchComputeText"];
  };
  "/BatchComputeJSON": {
    /**
     * BatchComputeJSON
     * @description Compute JSON for multiple prompts in batch using a language model.
     */
    post: operations["BatchComputeJSON"];
  };
  "/ComputeJSON": {
    /**
     * ComputeJSON
     * @description Compute JSON using a language model.
     */
    post: operations["ComputeJSON"];
  };
  "/MultiComputeJSON": {
    /**
     * MultiComputeJSON
     * @description Compute multiple JSON choices using a language model.
     */
    post: operations["MultiComputeJSON"];
  };
  "/Mistral7BInstruct": {
    /**
     * Mistral7BInstruct
     * @description Compute text using [Mistral 7B Instruct](https://mistral.ai/news/announcing-mistral-7b).
     */
    post: operations["Mistral7BInstruct"];
  };
  "/Mixtral8x7BInstruct": {
    /**
     * Mixtral8x7BInstruct
     * @description Compute text using instruct-tuned [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts/).
     */
    post: operations["Mixtral8x7BInstruct"];
  };
  "/Llama3Instruct8B": {
    /**
     * Llama3Instruct8B
     * @description Compute text using instruct-tuned [Llama 3 8B](https://llama.meta.com/llama3/).
     */
    post: operations["Llama3Instruct8B"];
  };
  "/Llama3Instruct70B": {
    /**
     * Llama3Instruct70B
     * @description Compute text using instruct-tuned [Llama 3 70B](https://llama.meta.com/llama3/).
     */
    post: operations["Llama3Instruct70B"];
  };
  "/Firellava13B": {
    /**
     * Firellava13B
     * @description Compute text with image input using [FireLLaVA 13B](https://fireworks.ai/blog/firellava-the-first-commercially-permissive-oss-llava-model).
     */
    post: operations["Firellava13B"];
  };
  "/GenerateImage": {
    /**
     * GenerateImage
     * @description Generate an image.
     */
    post: operations["GenerateImage"];
  };
  "/MultiGenerateImage": {
    /**
     * MultiGenerateImage
     * @description Generate multiple images.
     */
    post: operations["MultiGenerateImage"];
  };
  "/InpaintImage": {
    /**
     * InpaintImage
     * @description Edit an image using image generation inside part of the image or the full image.
     */
    post: operations["InpaintImage"];
  };
  "/MultiInpaintImage": {
    /**
     * MultiInpaintImage
     * @description Edit multiple images using image generation.
     */
    post: operations["MultiInpaintImage"];
  };
  "/StableDiffusionXLLightning": {
    /**
     * StableDiffusionXLLightning
     * @description Generate an image using [Stable Diffusion XL Lightning](https://arxiv.org/abs/2402.13929).
     */
    post: operations["StableDiffusionXLLightning"];
  };
  "/StableDiffusionXLInpaint": {
    /**
     * StableDiffusionXLInpaint
     * @description Edit an image using [Stable Diffusion XL](https://arxiv.org/abs/2307.01952). Supports inpainting (edit part of the image with a mask) and image-to-image (edit the full image).
     */
    post: operations["StableDiffusionXLInpaint"];
  };
  "/StableDiffusionXLControlNet": {
    /**
     * StableDiffusionXLControlNet
     * @description Generate an image with generation structured by an input image, using Stable Diffusion XL with [ControlNet](https://arxiv.org/abs/2302.05543).
     */
    post: operations["StableDiffusionXLControlNet"];
  };
  "/StableVideoDiffusion": {
    /**
     * StableVideoDiffusion
     * @description Generates a video using a still image as conditioning frame.
     */
    post: operations["StableVideoDiffusion"];
  };
  "/InterpolateFrames": {
    /**
     * InterpolateFrames
     * @description Generates a interpolation frames between each adjacent frames.
     */
    post: operations["InterpolateFrames"];
  };
  "/TranscribeSpeech": {
    /**
     * TranscribeSpeech
     * @description Transcribe speech in an audio or video file.
     */
    post: operations["TranscribeSpeech"];
  };
  "/GenerateSpeech": {
    /**
     * GenerateSpeech
     * @description Generate speech from text.
     */
    post: operations["GenerateSpeech"];
  };
  "/RemoveBackground": {
    /**
     * RemoveBackground
     * @description Remove the background from an image and return the foreground segment as a cut-out or a mask.
     */
    post: operations["RemoveBackground"];
  };
  "/EraseImage": {
    /**
     * EraseImage
     * @description Erase the masked part of an image, e.g. to remove an object by inpainting.
     */
    post: operations["EraseImage"];
  };
  "/UpscaleImage": {
    /**
     * UpscaleImage
     * @description Upscale an image using image generation.
     */
    post: operations["UpscaleImage"];
  };
  "/SegmentUnderPoint": {
    /**
     * SegmentUnderPoint
     * @description Segment an image under a point and return the segment.
     */
    post: operations["SegmentUnderPoint"];
  };
  "/SegmentAnything": {
    /**
     * SegmentAnything
     * @description Segment an image using [SegmentAnything](https://github.com/facebookresearch/segment-anything).
     */
    post: operations["SegmentAnything"];
  };
  "/SplitDocument": {
    /**
     * SplitDocument
     * @description Split document into text segments.
     */
    post: operations["SplitDocument"];
  };
  "/EmbedText": {
    /**
     * EmbedText
     * @description Generate embedding for a text document.
     */
    post: operations["EmbedText"];
  };
  "/MultiEmbedText": {
    /**
     * MultiEmbedText
     * @description Generate embeddings for multiple text documents.
     */
    post: operations["MultiEmbedText"];
  };
  "/EmbedImage": {
    /**
     * EmbedImage
     * @description Generate embedding for an image.
     */
    post: operations["EmbedImage"];
  };
  "/MultiEmbedImage": {
    /**
     * MultiEmbedImage
     * @description Generate embeddings for multiple images.
     */
    post: operations["MultiEmbedImage"];
  };
  "/JinaV2": {
    /**
     * JinaV2
     * @description Generate embeddings for multiple text documents using [Jina Embeddings 2](https://arxiv.org/abs/2310.19923).
     */
    post: operations["JinaV2"];
  };
  "/CLIP": {
    /**
     * CLIP
     * @description Generate embeddings for text or images using [CLIP](https://openai.com/research/clip).
     */
    post: operations["CLIP"];
  };
  "/FindOrCreateVectorStore": {
    /**
     * FindOrCreateVectorStore
     * @description Find a vector store matching the given collection name, or create a new vector store.
     */
    post: operations["FindOrCreateVectorStore"];
  };
  "/ListVectorStores": {
    /**
     * ListVectorStores
     * @description List all vector stores.
     */
    post: operations["ListVectorStores"];
  };
  "/DeleteVectorStore": {
    /**
     * DeleteVectorStore
     * @description Delete a vector store.
     */
    post: operations["DeleteVectorStore"];
  };
  "/QueryVectorStore": {
    /**
     * QueryVectorStore
     * @description Query a vector store for similar vectors.
     */
    post: operations["QueryVectorStore"];
  };
  "/FetchVectors": {
    /**
     * FetchVectors
     * @description Fetch vectors from a vector store.
     */
    post: operations["FetchVectors"];
  };
  "/UpdateVectors": {
    /**
     * UpdateVectors
     * @description Update vectors in a vector store.
     */
    post: operations["UpdateVectors"];
  };
  "/DeleteVectors": {
    /**
     * DeleteVectors
     * @description Delete vectors in a vector store.
     */
    post: operations["DeleteVectors"];
  };
}

export type webhooks = Record<string, never>;

export interface components {
  schemas: {
    /** ErrorOut */
    ErrorOut: {
      /**
       * @description The type of error returned.
       * @enum {string}
       */
      type: "api_error" | "invalid_request_error" | "dependency_error";
      /** @description A message providing more details about the error. */
      message: string;
      /**
       * @description The HTTP status code for the error.
       * @default 500
       */
      status_code?: number;
    };
    /** ExperimentalIn */
    ExperimentalIn: {
      /** @description Identifier. */
      name: string;
      /** @description Arguments. */
      args: {
        [key: string]: unknown;
      };
      /**
       * @description Timeout in seconds.
       * @default 60
       */
      timeout?: number;
    };
    /** ExperimentalOut */
    ExperimentalOut: {
      /** @description Response. */
      output: {
        [key: string]: unknown;
      };
    };
    /** BoxIn */
    BoxIn: {
      /** @description Values to box. */
      value: unknown;
    };
    /** BoxOut */
    BoxOut: {
      /** @description The evaluated result. */
      value: unknown;
    };
    /** IfIn */
    IfIn: {
      /** @description Condition. */
      condition: boolean;
      /** @description Result when condition is true. */
      value_if_true: unknown;
      /** @description Result when condition is false. */
      value_if_false?: unknown;
    };
    /** IfOut */
    IfOut: {
      /** @description Result. Null if `value_if_false` is not provided and `condition` is false. */
      result: unknown;
    };
    /** RunPythonIn */
    RunPythonIn: {
      /** @description Pickled function. */
      pkl_function?: string;
      /** @description Keyword arguments to your function. */
      kwargs: {
        [key: string]: unknown;
      };
      /** @description Python version. */
      python_version?: string;
      /** @description Python packages to install. You must import them in your code. */
      pip_install?: string[];
    };
    /** RunPythonOut */
    RunPythonOut: {
      /** @description Return value of your function. */
      output?: unknown;
      /** @description Pickled return value. */
      pkl_output?: string;
      /** @description Everything printed to stdout while running your code. */
      stdout: string;
      /** @description Contents of stderr if your code did not run successfully. */
      stderr: string;
    };
    /** ComputeTextIn */
    ComputeTextIn: {
      /** @description Input prompt. */
      prompt: string;
      /** @description Image prompts. */
      image_uris?: string[];
      /**
       * Format: float
       * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
       * @default 0.4
       */
      temperature?: number;
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
      /**
       * @description Selected model. `Firellava13B` is automatically selected when `image_uris` is provided.
       * @default Llama3Instruct8B
       * @enum {string}
       */
      model?:
        | "Mistral7BInstruct"
        | "Mixtral8x7BInstruct"
        | "Llama3Instruct8B"
        | "Llama3Instruct70B"
        | "Llama3Instruct405B"
        | "Firellava13B"
        | "gpt-4o"
        | "gpt-4o-mini"
        | "claude-3-5-sonnet-20240620";
    };
    /** ComputeTextOut */
    ComputeTextOut: {
      /** @description Text response. */
      text: string;
    };
    /** ComputeJSONIn */
    ComputeJSONIn: {
      /** @description Input prompt. */
      prompt: string;
      /** @description JSON schema to guide `json_object` response. */
      json_schema: {
        [key: string]: unknown;
      };
      /**
       * Format: float
       * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
       * @default 0.4
       */
      temperature?: number;
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
      /**
       * @description Selected model.
       * @default Llama3Instruct8B
       * @enum {string}
       */
      model?: "Mistral7BInstruct" | "Mixtral8x7BInstruct" | "Llama3Instruct8B";
    };
    /** ComputeJSONOut */
    ComputeJSONOut: {
      /** @description JSON response. */
      json_object?: {
        [key: string]: unknown;
      };
      /** @description If the model output could not be parsed to JSON, this is the raw text output. */
      text?: string;
    };
    /** MultiComputeTextIn */
    MultiComputeTextIn: {
      /** @description Input prompt. */
      prompt: string;
      /**
       * @description Number of choices to generate.
       * @default 1
       */
      num_choices: number;
      /**
       * Format: float
       * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
       * @default 0.4
       */
      temperature?: number;
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
      /**
       * @description Selected model.
       * @default Llama3Instruct8B
       * @enum {string}
       */
      model?:
        | "Mistral7BInstruct"
        | "Mixtral8x7BInstruct"
        | "Llama3Instruct8B"
        | "Llama3Instruct70B";
    };
    /** MultiComputeTextOut */
    MultiComputeTextOut: {
      /** @description Response choices. */
      choices: {
        /** @description Text response. */
        text: string;
      }[];
    };
    /** BatchComputeTextIn */
    BatchComputeTextIn: {
      /** @description Batch input prompts. */
      prompts: string[];
      /**
       * Format: float
       * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
       * @default 0.4
       */
      temperature?: number;
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
      /**
       * @description Selected model.
       * @default Llama3Instruct8B
       * @enum {string}
       */
      model?: "Mistral7BInstruct" | "Llama3Instruct8B";
    };
    /** BatchComputeTextOut */
    BatchComputeTextOut: {
      /** @description Batch outputs. */
      outputs: {
        /** @description Text response. */
        text: string;
      }[];
    };
    /** MultiComputeJSONIn */
    MultiComputeJSONIn: {
      /** @description Input prompt. */
      prompt: string;
      /** @description JSON schema to guide `json_object` response. */
      json_schema: {
        [key: string]: unknown;
      };
      /**
       * @description Number of choices to generate.
       * @default 2
       */
      num_choices: number;
      /**
       * Format: float
       * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
       * @default 0.4
       */
      temperature?: number;
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
      /**
       * @description Selected model.
       * @default Llama3Instruct8B
       * @enum {string}
       */
      model?: "Mistral7BInstruct" | "Mixtral8x7BInstruct" | "Llama3Instruct8B";
    };
    /** MultiComputeJSONOut */
    MultiComputeJSONOut: {
      /** @description Response choices. */
      choices: {
        /** @description JSON response. */
        json_object?: {
          [key: string]: unknown;
        };
        /** @description If the model output could not be parsed to JSON, this is the raw text output. */
        text?: string;
      }[];
    };
    /** BatchComputeJSONIn */
    BatchComputeJSONIn: {
      /** @description Batch input prompts. */
      prompts: string[];
      /** @description JSON schema to guide `json_object` response. */
      json_schema: {
        [key: string]: unknown;
      };
      /**
       * Format: float
       * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
       * @default 0.4
       */
      temperature?: number;
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
      /**
       * @description Selected model.
       * @default Llama3Instruct8B
       * @enum {string}
       */
      model?: "Mistral7BInstruct" | "Llama3Instruct8B";
    };
    /** BatchComputeJSONOut */
    BatchComputeJSONOut: {
      /** @description Batch outputs. */
      outputs: {
        /** @description JSON response. */
        json_object?: {
          [key: string]: unknown;
        };
        /** @description If the model output could not be parsed to JSON, this is the raw text output. */
        text?: string;
      }[];
    };
    /** Mistral7BInstructIn */
    Mistral7BInstructIn: {
      /** @description Input prompt. */
      prompt: string;
      /** @description System prompt. */
      system_prompt?: string;
      /**
       * @description Number of choices to generate.
       * @default 1
       */
      num_choices?: number;
      /** @description JSON schema to guide response. */
      json_schema?: {
        [key: string]: unknown;
      };
      /**
       * Format: float
       * @description Higher values make the output more random, lower values make the output more deterministic.
       */
      temperature?: number;
      /**
       * Format: float
       * @description Higher values decrease the likelihood of repeating previous tokens.
       * @default 0
       */
      frequency_penalty?: number;
      /**
       * Format: float
       * @description Higher values decrease the likelihood of repeated sequences.
       * @default 1
       */
      repetition_penalty?: number;
      /**
       * Format: float
       * @description Higher values increase the likelihood of new topics appearing.
       * @default 1.1
       */
      presence_penalty?: number;
      /**
       * Format: float
       * @description Probability below which less likely tokens are filtered out.
       * @default 0.95
       */
      top_p?: number;
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
    };
    /** Mistral7BInstructChoice */
    Mistral7BInstructChoice: {
      /** @description Text response, if `json_schema` was not provided. */
      text?: string;
      /** @description JSON response, if `json_schema` was provided. */
      json_object?: {
        [key: string]: unknown;
      };
    };
    /** Mistral7BInstructOut */
    Mistral7BInstructOut: {
      /** @description Response choices. */
      choices: {
        /** @description Text response, if `json_schema` was not provided. */
        text?: string;
        /** @description JSON response, if `json_schema` was provided. */
        json_object?: {
          [key: string]: unknown;
        };
      }[];
    };
    /** Mixtral8x7BInstructIn */
    Mixtral8x7BInstructIn: {
      /** @description Input prompt. */
      prompt: string;
      /** @description System prompt. */
      system_prompt?: string;
      /**
       * @description Number of choices to generate.
       * @default 1
       */
      num_choices?: number;
      /** @description JSON schema to guide response. */
      json_schema?: {
        [key: string]: unknown;
      };
      /**
       * Format: float
       * @description Higher values make the output more random, lower values make the output more deterministic.
       */
      temperature?: number;
      /**
       * Format: float
       * @description Higher values decrease the likelihood of repeating previous tokens.
       * @default 0
       */
      frequency_penalty?: number;
      /**
       * Format: float
       * @description Higher values decrease the likelihood of repeated sequences.
       * @default 1
       */
      repetition_penalty?: number;
      /**
       * Format: float
       * @description Higher values increase the likelihood of new topics appearing.
       * @default 1.1
       */
      presence_penalty?: number;
      /**
       * Format: float
       * @description Probability below which less likely tokens are filtered out.
       * @default 0.95
       */
      top_p?: number;
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
    };
    /** Mixtral8x7BChoice */
    Mixtral8x7BChoice: {
      /** @description Text response, if `json_schema` was not provided. */
      text?: string;
      /** @description JSON response, if `json_schema` was provided. */
      json_object?: {
        [key: string]: unknown;
      };
    };
    /** Mixtral8x7BInstructOut */
    Mixtral8x7BInstructOut: {
      /** @description Response choices. */
      choices: {
        /** @description Text response, if `json_schema` was not provided. */
        text?: string;
        /** @description JSON response, if `json_schema` was provided. */
        json_object?: {
          [key: string]: unknown;
        };
      }[];
    };
    /** Llama3Instruct8BIn */
    Llama3Instruct8BIn: {
      /** @description Input prompt. */
      prompt: string;
      /** @description System prompt. */
      system_prompt?: string;
      /**
       * @description Number of choices to generate.
       * @default 1
       */
      num_choices?: number;
      /**
       * Format: float
       * @description Higher values make the output more random, lower values make the output more deterministic.
       */
      temperature?: number;
      /**
       * Format: float
       * @description Higher values decrease the likelihood of repeating previous tokens.
       * @default 0
       */
      frequency_penalty?: number;
      /**
       * Format: float
       * @description Higher values decrease the likelihood of repeated sequences.
       * @default 1
       */
      repetition_penalty?: number;
      /**
       * Format: float
       * @description Higher values increase the likelihood of new topics appearing.
       * @default 1.1
       */
      presence_penalty?: number;
      /**
       * Format: float
       * @description Probability below which less likely tokens are filtered out.
       * @default 0.95
       */
      top_p?: number;
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
      /** @description JSON schema to guide response. */
      json_schema?: {
        [key: string]: unknown;
      };
    };
    /** Llama3Instruct8BChoice */
    Llama3Instruct8BChoice: {
      /** @description Text response. */
      text?: string;
      /** @description JSON response, if `json_schema` was provided. */
      json_object?: {
        [key: string]: unknown;
      };
    };
    /** Llama3Instruct8BOut */
    Llama3Instruct8BOut: {
      /** @description Response choices. */
      choices: {
        /** @description Text response. */
        text?: string;
        /** @description JSON response, if `json_schema` was provided. */
        json_object?: {
          [key: string]: unknown;
        };
      }[];
    };
    /** Llama3Instruct70BIn */
    Llama3Instruct70BIn: {
      /** @description Input prompt. */
      prompt: string;
      /** @description System prompt. */
      system_prompt?: string;
      /**
       * @description Number of choices to generate.
       * @default 1
       */
      num_choices?: number;
      /**
       * Format: float
       * @description Higher values make the output more random, lower values make the output more deterministic.
       */
      temperature?: number;
      /**
       * Format: float
       * @description Higher values decrease the likelihood of repeating previous tokens.
       * @default 0
       */
      frequency_penalty?: number;
      /**
       * Format: float
       * @description Higher values decrease the likelihood of repeated sequences.
       * @default 1
       */
      repetition_penalty?: number;
      /**
       * Format: float
       * @description Higher values increase the likelihood of new topics appearing.
       * @default 1.1
       */
      presence_penalty?: number;
      /**
       * Format: float
       * @description Probability below which less likely tokens are filtered out.
       * @default 0.95
       */
      top_p?: number;
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
    };
    /** Llama3Instruct70BChoice */
    Llama3Instruct70BChoice: {
      /** @description Text response. */
      text?: string;
    };
    /** Llama3Instruct70BOut */
    Llama3Instruct70BOut: {
      /** @description Response choices. */
      choices: {
        /** @description Text response. */
        text?: string;
      }[];
    };
    /** Firellava13BIn */
    Firellava13BIn: {
      /** @description Text prompt. */
      prompt: string;
      /** @description Image prompts. */
      image_uris: string[];
      /** @description Maximum number of tokens to generate. */
      max_tokens?: number;
    };
    /** Firellava13BOut */
    Firellava13BOut: {
      /** @description Text response. */
      text: string;
    };
    /** GenerateImageIn */
    GenerateImageIn: {
      /** @description Text prompt. */
      prompt: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** GenerateImageOut */
    GenerateImageOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
    };
    /** MultiGenerateImageIn */
    MultiGenerateImageIn: {
      /** @description Text prompt. */
      prompt: string;
      /**
       * @description Number of images to generate.
       * @default 2
       */
      num_images: number;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** MultiGenerateImageOut */
    MultiGenerateImageOut: {
      /** @description Generated images. */
      outputs: {
        /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
        image_uri: string;
      }[];
    };
    /** StableDiffusionXLIn */
    StableDiffusionXLIn: {
      /** @description Text prompt. */
      prompt: string;
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /**
       * @description Number of diffusion steps.
       * @default 30
       */
      steps?: number;
      /**
       * @description Number of images to generate.
       * @default 1
       */
      num_images: number;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /**
       * @description Height of output image, in pixels.
       * @default 1024
       */
      height?: number;
      /**
       * @description Width of output image, in pixels.
       * @default 1024
       */
      width?: number;
      /** @description Seeds for deterministic generation. Default is a random seed. */
      seeds?: number[];
      /**
       * Format: float
       * @description Higher values adhere to the text prompt more strongly, typically at the expense of image quality.
       * @default 7
       */
      guidance_scale?: number;
    };
    /** StableDiffusionImage */
    StableDiffusionImage: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
      /** @description The random noise seed used for generation. */
      seed: number;
    };
    /** StableDiffusionXLOut */
    StableDiffusionXLOut: {
      /** @description Generated images. */
      outputs: {
        /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
        image_uri: string;
        /** @description The random noise seed used for generation. */
        seed: number;
      }[];
    };
    /** StableDiffusionXLLightningIn */
    StableDiffusionXLLightningIn: {
      /** @description Text prompt. */
      prompt: string;
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /**
       * @description Number of images to generate.
       * @default 1
       */
      num_images?: number;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /**
       * @description Height of output image, in pixels.
       * @default 1024
       */
      height?: number;
      /**
       * @description Width of output image, in pixels.
       * @default 1024
       */
      width?: number;
      /** @description Seeds for deterministic generation. Default is a random seed. */
      seeds?: number[];
    };
    /** StableDiffusionXLLightningOut */
    StableDiffusionXLLightningOut: {
      /** @description Generated images. */
      outputs: {
        /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
        image_uri: string;
        /** @description The random noise seed used for generation. */
        seed: number;
      }[];
    };
    /** StableDiffusionXLIPAdapterIn */
    StableDiffusionXLIPAdapterIn: {
      /** @description Text prompt. */
      prompt: string;
      /** @description Image prompt. */
      image_prompt_uri: string;
      /**
       * @description Number of images to generate.
       * @default 1
       */
      num_images: number;
      /**
       * Format: float
       * @description Controls the influence of the image prompt on the generated output.
       * @default 0.5
       */
      ip_adapter_scale?: number;
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /**
       * @description Width of output image, in pixels.
       * @default 1024
       */
      width?: number;
      /**
       * @description Height of output image, in pixels.
       * @default 1024
       */
      height?: number;
      /** @description Random noise seeds. Default is random seeds for each generation. */
      seeds?: number[];
    };
    /** StableDiffusionXLIPAdapterOut */
    StableDiffusionXLIPAdapterOut: {
      /** @description Generated images. */
      outputs: {
        /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
        image_uri: string;
        /** @description The random noise seed used for generation. */
        seed: number;
      }[];
    };
    /** StableDiffusionXLControlNetIn */
    StableDiffusionXLControlNetIn: {
      /** @description Input image. */
      image_uri: string;
      /**
       * @description Strategy to control generation using the input image.
       * @enum {string}
       */
      control_method: "edge" | "depth" | "illusion" | "tile";
      /** @description Text prompt. */
      prompt: string;
      /**
       * @description Number of images to generate.
       * @default 1
       */
      num_images: number;
      /**
       * @description Resolution of the output image, in pixels.
       * @default 1024
       */
      output_resolution?: number;
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /**
       * Format: float
       * @description Controls the influence of the input image on the generated output.
       * @default 0.5
       */
      conditioning_scale?: number;
      /**
       * Format: float
       * @description Controls how much to transform the input image.
       * @default 0.5
       */
      strength?: number;
      /** @description Random noise seeds. Default is random seeds for each generation. */
      seeds?: number[];
    };
    /** StableDiffusionXLControlNetOut */
    StableDiffusionXLControlNetOut: {
      /** @description Generated images. */
      outputs: {
        /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
        image_uri: string;
        /** @description The random noise seed used for generation. */
        seed: number;
      }[];
    };
    /** StableVideoDiffusionIn */
    StableVideoDiffusionIn: {
      /** @description Original image. */
      image_uri: string;
      /** @description Use "hosted" to return a video URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the video data will be returned as a base64-encoded string. */
      store?: string;
      /**
       * @description Output video format.
       * @default gif
       * @enum {string}
       */
      output_format?: "gif" | "webp" | "mp4" | "frames";
      /** @description Seed for deterministic generation. Default is a random seed. */
      seed?: number;
      /**
       * @description Frames per second of the generated video. Ignored if output format is `frames`.
       * @default 7
       */
      fps?: number;
      /**
       * @description The motion bucket id to use for the generated video. This can be used to control the motion of the generated video. Increasing the motion bucket id increases the motion of the generated video.
       * @default 180
       */
      motion_bucket_id?: number;
      /**
       * Format: float
       * @description The amount of noise added to the conditioning image. The higher the values the less the video resembles the conditioning image. Increasing this value also increases the motion of the generated video.
       * @default 0.1
       */
      noise?: number;
    };
    /** StableVideoDiffusionOut */
    StableVideoDiffusionOut: {
      /** @description Generated video. */
      video_uri?: string;
      /** @description Generated frames. */
      frame_uris?: string[];
    };
    /** InterpolateFramesIn */
    InterpolateFramesIn: {
      /** @description Frames. */
      frame_uris: string[];
      /** @description Use "hosted" to return a video URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the video data will be returned as a base64-encoded string. */
      store?: string;
      /**
       * @description Output video format.
       * @default gif
       * @enum {string}
       */
      output_format?: "gif" | "webp" | "mp4" | "frames";
      /**
       * @description Frames per second of the generated video. Ignored if output format is `frames`.
       * @default 7
       */
      fps?: number;
      /**
       * @description Number of interpolation steps. Each step adds an interpolated frame between adjacent frames. For example, 2 steps over 2 frames produces 5 frames.
       * @default 2
       */
      num_steps?: number;
    };
    /** InterpolateFramesOut */
    InterpolateFramesOut: {
      /** @description Generated video. */
      video_uri?: string;
      /** @description Output frames. */
      frame_uris?: string[];
    };
    /** InpaintImageIn */
    InpaintImageIn: {
      /** @description Original image. */
      image_uri: string;
      /** @description Text prompt. */
      prompt: string;
      /** @description Mask image that controls which pixels are inpainted. If unset, the entire image is edited (image-to-image). */
      mask_image_uri?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** InpaintImageOut */
    InpaintImageOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
    };
    /** MultiInpaintImageIn */
    MultiInpaintImageIn: {
      /** @description Original image. */
      image_uri: string;
      /** @description Text prompt. */
      prompt: string;
      /** @description Mask image that controls which pixels are edited (inpainting). If unset, the entire image is edited (image-to-image). */
      mask_image_uri?: string;
      /**
       * @description Number of images to generate.
       * @default 2
       */
      num_images: number;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** MultiInpaintImageOut */
    MultiInpaintImageOut: {
      /** @description Generated images. */
      outputs: {
        /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
        image_uri: string;
      }[];
    };
    /** StableDiffusionXLInpaintIn */
    StableDiffusionXLInpaintIn: {
      /** @description Original image. */
      image_uri: string;
      /** @description Text prompt. */
      prompt: string;
      /** @description Mask image that controls which pixels are edited (inpainting). If unset, the entire image is edited (image-to-image). */
      mask_image_uri?: string;
      /**
       * @description Number of images to generate.
       * @default 1
       */
      num_images: number;
      /**
       * @description Resolution of the output image, in pixels.
       * @default 1024
       */
      output_resolution?: number;
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /**
       * Format: float
       * @description Controls the strength of the generation process.
       * @default 0.8
       */
      strength?: number;
      /** @description Random noise seeds. Default is random seeds for each generation. */
      seeds?: number[];
    };
    /** StableDiffusionXLInpaintOut */
    StableDiffusionXLInpaintOut: {
      /** @description Generated images. */
      outputs: {
        /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
        image_uri: string;
        /** @description The random noise seed used for generation. */
        seed: number;
      }[];
    };
    /** BoundingBox */
    BoundingBox: {
      /**
       * Format: float
       * @description Top left corner x.
       */
      x1: number;
      /**
       * Format: float
       * @description Top left corner y.
       */
      y1: number;
      /**
       * Format: float
       * @description Bottom right corner x.
       */
      x2: number;
      /**
       * Format: float
       * @description Bottom right corner y.
       */
      y2: number;
    };
    /** Point */
    Point: {
      /** @description X position. */
      x: number;
      /** @description Y position. */
      y: number;
    };
    /** EraseImageIn */
    EraseImageIn: {
      /** @description Input image. */
      image_uri: string;
      /** @description Mask image that controls which pixels are inpainted. */
      mask_image_uri: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** EraseImageOut */
    EraseImageOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
    };
    /** BigLaMaIn */
    BigLaMaIn: {
      /** @description Input image. */
      image_uri: string;
      /** @description Mask image that controls which pixels are inpainted. */
      mask_image_uri: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** BigLaMaOut */
    BigLaMaOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
    };
    /** RemoveBackgroundIn */
    RemoveBackgroundIn: {
      /** @description Input image. */
      image_uri: string;
      /**
       * @description Return a mask image instead of the original content.
       * @default false
       */
      return_mask?: boolean;
      /**
       * @description Invert the mask image. Only takes effect if `return_mask` is true.
       * @default false
       */
      invert_mask?: boolean;
      /** @description Hex value background color. Transparent if unset. */
      background_color?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** RemoveBackgroundOut */
    RemoveBackgroundOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
    };
    /** DISISNetIn */
    DISISNetIn: {
      /** @description Input image. */
      image_uri: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** DISISNetOut */
    DISISNetOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
    };
    /** UpscaleImageIn */
    UpscaleImageIn: {
      /** @description Prompt to guide model on the content of image to upscale. */
      prompt?: string;
      /** @description Input image. */
      image_uri: string;
      /**
       * @description Resolution of the output image, in pixels.
       * @default 1024
       */
      output_resolution?: number;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** UpscaleImageOut */
    UpscaleImageOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
    };
    /** SegmentUnderPointIn */
    SegmentUnderPointIn: {
      /** @description Input image. */
      image_uri: string;
      /** Point */
      point: {
        /** @description X position. */
        x: number;
        /** @description Y position. */
        y: number;
      };
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** SegmentUnderPointOut */
    SegmentUnderPointOut: {
      /** @description Detected segments in 'mask image' format. Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      mask_image_uri: string;
    };
    /** SegmentAnythingIn */
    SegmentAnythingIn: {
      /** @description Input image. */
      image_uri: string;
      /** @description Point prompts, to detect a segment under the point. One of `point_prompts` or `box_prompts` must be set. */
      point_prompts?: {
        /** @description X position. */
        x: number;
        /** @description Y position. */
        y: number;
      }[];
      /** @description Box prompts, to detect a segment within the bounding box. One of `point_prompts` or `box_prompts` must be set. */
      box_prompts?: {
        /**
         * Format: float
         * @description Top left corner x.
         */
        x1: number;
        /**
         * Format: float
         * @description Top left corner y.
         */
        y1: number;
        /**
         * Format: float
         * @description Bottom right corner x.
         */
        x2: number;
        /**
         * Format: float
         * @description Bottom right corner y.
         */
        y2: number;
      }[];
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** SegmentAnythingOut */
    SegmentAnythingOut: {
      /** @description Detected segments in 'mask image' format. Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      mask_image_uri: string;
    };
    /** TranscribeSpeechIn */
    TranscribeSpeechIn: {
      /** @description Input audio. */
      audio_uri: string;
      /** @description Prompt to guide model on the content and context of input audio. */
      prompt?: string;
      /**
       * @description Language of input audio in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format.
       * @default en
       */
      language?: string;
      /**
       * @description Segment the text into sentences with approximate timestamps.
       * @default false
       */
      segment?: boolean;
      /**
       * @description Align transcription to produce more accurate sentence-level timestamps and word-level timestamps. An array of word segments will be included in each sentence segment.
       * @default false
       */
      align?: boolean;
      /**
       * @description Identify speakers for each segment. Speaker IDs will be included in each segment.
       * @default false
       */
      diarize?: boolean;
      /**
       * @description Suggest automatic chapter markers.
       * @default false
       */
      suggest_chapters?: boolean;
    };
    /** TranscribedWord */
    TranscribedWord: {
      /** @description Text of word. */
      word: string;
      /**
       * Format: float
       * @description Start time of word, in seconds.
       */
      start?: number;
      /**
       * Format: float
       * @description End time of word, in seconds.
       */
      end?: number;
      /** @description ID of speaker, if `diarize` is enabled. */
      speaker?: string;
    };
    /** TranscribedSegment */
    TranscribedSegment: {
      /** @description Text of segment. */
      text: string;
      /**
       * Format: float
       * @description Start time of segment, in seconds.
       */
      start: number;
      /**
       * Format: float
       * @description End time of segment, in seconds.
       */
      end: number;
      /** @description ID of speaker, if `diarize` is enabled. */
      speaker?: string;
      /** @description Aligned words, if `align` is enabled. */
      words?: {
        /** @description Text of word. */
        word: string;
        /**
         * Format: float
         * @description Start time of word, in seconds.
         */
        start?: number;
        /**
         * Format: float
         * @description End time of word, in seconds.
         */
        end?: number;
        /** @description ID of speaker, if `diarize` is enabled. */
        speaker?: string;
      }[];
    };
    /** ChapterMarker */
    ChapterMarker: {
      /** @description Chapter title. */
      title: string;
      /**
       * Format: float
       * @description Start time of chapter, in seconds.
       */
      start: number;
    };
    /** TranscribeSpeechOut */
    TranscribeSpeechOut: {
      /** @description Transcribed text. */
      text: string;
      /** @description Transcribed segments, if `segment` is enabled. */
      segments?: {
        /** @description Text of segment. */
        text: string;
        /**
         * Format: float
         * @description Start time of segment, in seconds.
         */
        start: number;
        /**
         * Format: float
         * @description End time of segment, in seconds.
         */
        end: number;
        /** @description ID of speaker, if `diarize` is enabled. */
        speaker?: string;
        /** @description Aligned words, if `align` is enabled. */
        words?: {
          /** @description Text of word. */
          word: string;
          /**
           * Format: float
           * @description Start time of word, in seconds.
           */
          start?: number;
          /**
           * Format: float
           * @description End time of word, in seconds.
           */
          end?: number;
          /** @description ID of speaker, if `diarize` is enabled. */
          speaker?: string;
        }[];
      }[];
      /** @description Chapter markers, if `suggest_chapters` is enabled. */
      chapters?: {
        /** @description Chapter title. */
        title: string;
        /**
         * Format: float
         * @description Start time of chapter, in seconds.
         */
        start: number;
      }[];
    };
    /** GenerateSpeechIn */
    GenerateSpeechIn: {
      /** @description Input text. */
      text: string;
      /** @description Use "hosted" to return an audio URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the audio data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** GenerateSpeechOut */
    GenerateSpeechOut: {
      /** @description Base 64-encoded WAV audio bytes, or a hosted audio url if `store` is provided. */
      audio_uri: string;
    };
    /** XTTSV2In */
    XTTSV2In: {
      /** @description Input text. */
      text: string;
      /** @description Reference audio used to synthesize the speaker. If unset, a default speaker voice will be used. */
      audio_uri?: string;
      /**
       * @description Language of input text. Supported languages: `en, de, fr, es, it, pt, pl, zh, ar, cs, ru, nl, tr, hu, ko`.
       * @default en
       */
      language?: string;
      /** @description Use "hosted" to return an audio URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the audio data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** XTTSV2Out */
    XTTSV2Out: {
      /** @description Base 64-encoded WAV audio bytes, or a hosted audio url if `store` is provided. */
      audio_uri: string;
    };
    /** Embedding */
    Embedding: {
      /** @description Embedding vector. */
      vector: number[];
      /** @description Vector store document ID. */
      doc_id?: string;
      /** @description Vector store document metadata. */
      metadata?: {
        [key: string]: unknown;
      };
    };
    /** EmbedTextIn */
    EmbedTextIn: {
      /** @description Text to embed. */
      text: string;
      /** @description Vector store name. */
      collection_name?: string;
      /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
      metadata?: {
        [key: string]: unknown;
      };
      /** @description Choose keys from `metadata` to embed with text. */
      embedded_metadata_keys?: string[];
      /** @description Vector store document ID. Ignored if `store` is unset. */
      doc_id?: string;
      /**
       * @description Selected embedding model.
       * @default jina-v2
       * @enum {string}
       */
      model?: "jina-v2" | "clip";
    };
    /** EmbedTextOut */
    EmbedTextOut: {
      /** Embedding */
      embedding: {
        /** @description Embedding vector. */
        vector: number[];
        /** @description Vector store document ID. */
        doc_id?: string;
        /** @description Vector store document metadata. */
        metadata?: {
          [key: string]: unknown;
        };
      };
    };
    /** EmbedTextItem */
    EmbedTextItem: {
      /** @description Text to embed. */
      text: string;
      /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
      metadata?: {
        [key: string]: unknown;
      };
      /** @description Vector store document ID. Ignored if `collection_name` is unset. */
      doc_id?: string;
    };
    /** MultiEmbedTextIn */
    MultiEmbedTextIn: {
      /** @description Items to embed. */
      items: {
        /** @description Text to embed. */
        text: string;
        /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
        metadata?: {
          [key: string]: unknown;
        };
        /** @description Vector store document ID. Ignored if `collection_name` is unset. */
        doc_id?: string;
      }[];
      /** @description Vector store name. */
      collection_name?: string;
      /** @description Choose keys from `metadata` to embed with text. */
      embedded_metadata_keys?: string[];
      /**
       * @description Selected embedding model.
       * @default jina-v2
       * @enum {string}
       */
      model?: "jina-v2" | "clip";
    };
    /** MultiEmbedTextOut */
    MultiEmbedTextOut: {
      /** @description Generated embeddings. */
      embeddings: {
        /** @description Embedding vector. */
        vector: number[];
        /** @description Vector store document ID. */
        doc_id?: string;
        /** @description Vector store document metadata. */
        metadata?: {
          [key: string]: unknown;
        };
      }[];
    };
    /** JinaV2In */
    JinaV2In: {
      /** @description Items to embed. */
      items: {
        /** @description Text to embed. */
        text: string;
        /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
        metadata?: {
          [key: string]: unknown;
        };
        /** @description Vector store document ID. Ignored if `collection_name` is unset. */
        doc_id?: string;
      }[];
      /** @description Vector store name. */
      collection_name?: string;
      /** @description Choose keys from `metadata` to embed with text. */
      embedded_metadata_keys?: string[];
    };
    /** JinaV2Out */
    JinaV2Out: {
      /** @description Generated embeddings. */
      embeddings: {
        /** @description Embedding vector. */
        vector: number[];
        /** @description Vector store document ID. */
        doc_id?: string;
        /** @description Vector store document metadata. */
        metadata?: {
          [key: string]: unknown;
        };
      }[];
    };
    /** EmbedImageIn */
    EmbedImageIn: {
      /** @description Image to embed. */
      image_uri: string;
      /** @description Vector store name. */
      collection_name?: string;
      /** @description Vector store document ID. Ignored if `collection_name` is unset. */
      doc_id?: string;
      /**
       * @description Selected embedding model.
       * @default clip
       * @enum {string}
       */
      model?: "clip";
    };
    /** EmbedImageOut */
    EmbedImageOut: {
      /** Embedding */
      embedding: {
        /** @description Embedding vector. */
        vector: number[];
        /** @description Vector store document ID. */
        doc_id?: string;
        /** @description Vector store document metadata. */
        metadata?: {
          [key: string]: unknown;
        };
      };
    };
    /** EmbedImageItem */
    EmbedImageItem: {
      /** @description Image to embed. */
      image_uri: string;
      /** @description Vector store document ID. Ignored if `collection_name` is unset. */
      doc_id?: string;
    };
    /** EmbedTextOrImageItem */
    EmbedTextOrImageItem: {
      /** @description Image to embed. */
      image_uri?: string;
      /** @description Text to embed. */
      text?: string;
      /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
      metadata?: {
        [key: string]: unknown;
      };
      /** @description Vector store document ID. Ignored if `collection_name` is unset. */
      doc_id?: string;
    };
    /** MultiEmbedImageIn */
    MultiEmbedImageIn: {
      /** @description Items to embed. */
      items: {
        /** @description Image to embed. */
        image_uri: string;
        /** @description Vector store document ID. Ignored if `collection_name` is unset. */
        doc_id?: string;
      }[];
      /** @description Vector store name. */
      collection_name?: string;
      /**
       * @description Selected embedding model.
       * @default clip
       * @enum {string}
       */
      model?: "clip";
    };
    /** MultiEmbedImageOut */
    MultiEmbedImageOut: {
      /** @description Generated embeddings. */
      embeddings: {
        /** @description Embedding vector. */
        vector: number[];
        /** @description Vector store document ID. */
        doc_id?: string;
        /** @description Vector store document metadata. */
        metadata?: {
          [key: string]: unknown;
        };
      }[];
    };
    /** CLIPIn */
    CLIPIn: {
      /** @description Items to embed. */
      items: {
        /** @description Image to embed. */
        image_uri?: string;
        /** @description Text to embed. */
        text?: string;
        /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
        metadata?: {
          [key: string]: unknown;
        };
        /** @description Vector store document ID. Ignored if `collection_name` is unset. */
        doc_id?: string;
      }[];
      /** @description Vector store name. */
      collection_name?: string;
      /** @description Choose keys from `metadata` to embed with text. Only applies to text items. */
      embedded_metadata_keys?: string[];
    };
    /** CLIPOut */
    CLIPOut: {
      /** @description Generated embeddings. */
      embeddings: {
        /** @description Embedding vector. */
        vector: number[];
        /** @description Vector store document ID. */
        doc_id?: string;
        /** @description Vector store document metadata. */
        metadata?: {
          [key: string]: unknown;
        };
      }[];
    };
    /** FindOrCreateVectorStoreIn */
    FindOrCreateVectorStoreIn: {
      /** @description Vector store name. */
      collection_name: string;
      /**
       * @description Selected embedding model.
       * @enum {string}
       */
      model: "jina-v2" | "clip";
    };
    /** FindOrCreateVectorStoreOut */
    FindOrCreateVectorStoreOut: {
      /** @description Vector store name. */
      collection_name: string;
      /**
       * @description Selected embedding model.
       * @enum {string}
       */
      model: "jina-v2" | "clip";
      /** @description Number of leaves in the vector store. */
      num_leaves?: number;
    };
    /** ListVectorStoresIn */
    ListVectorStoresIn: Record<string, never>;
    /** ListVectorStoresOut */
    ListVectorStoresOut: {
      /** @description List of vector stores. */
      items?: {
        /** @description Vector store name. */
        collection_name: string;
        /**
         * @description Selected embedding model.
         * @enum {string}
         */
        model: "jina-v2" | "clip";
        /** @description Number of leaves in the vector store. */
        num_leaves?: number;
      }[];
    };
    /** DeleteVectorStoreIn */
    DeleteVectorStoreIn: {
      /** @description Vector store name. */
      collection_name: string;
      /**
       * @description Selected embedding model.
       * @enum {string}
       */
      model: "jina-v2" | "clip";
    };
    /** DeleteVectorStoreOut */
    DeleteVectorStoreOut: {
      /** @description Vector store name. */
      collection_name: string;
      /**
       * @description Selected embedding model.
       * @enum {string}
       */
      model: "jina-v2" | "clip";
    };
    /**
     * Vector
     * @description Canonical representation of document with embedding vector.
     */
    Vector: {
      /** @description Document ID. */
      id: string;
      /** @description Embedding vector. */
      vector: number[];
      /** @description Document metadata. */
      metadata: {
        [key: string]: unknown;
      };
    };
    /** FetchVectorsIn */
    FetchVectorsIn: {
      /** @description Vector store name. */
      collection_name: string;
      /**
       * @description Selected embedding model.
       * @enum {string}
       */
      model: "jina-v2" | "clip";
      /** @description Document IDs to retrieve. */
      ids: string[];
    };
    /** FetchVectorsOut */
    FetchVectorsOut: {
      /** @description Retrieved vectors. */
      vectors: {
        /** @description Document ID. */
        id: string;
        /** @description Embedding vector. */
        vector: number[];
        /** @description Document metadata. */
        metadata: {
          [key: string]: unknown;
        };
      }[];
    };
    /** UpdateVectorsOut */
    UpdateVectorsOut: {
      /** @description Number of vectors modified. */
      count: number;
    };
    /** DeleteVectorsOut */
    DeleteVectorsOut: {
      /** @description Number of vectors modified. */
      count: number;
    };
    /** UpdateVectorParams */
    UpdateVectorParams: {
      /** @description Document ID. */
      id: string;
      /** @description Embedding vector. */
      vector?: number[];
      /** @description Document metadata. */
      metadata?: {
        [key: string]: unknown;
      };
    };
    /** UpdateVectorsIn */
    UpdateVectorsIn: {
      /** @description Vector store name. */
      collection_name: string;
      /**
       * @description Selected embedding model.
       * @enum {string}
       */
      model: "jina-v2" | "clip";
      /** @description Vectors to upsert. */
      vectors: {
        /** @description Document ID. */
        id: string;
        /** @description Embedding vector. */
        vector?: number[];
        /** @description Document metadata. */
        metadata?: {
          [key: string]: unknown;
        };
      }[];
    };
    /** DeleteVectorsIn */
    DeleteVectorsIn: {
      /** @description Vector store name. */
      collection_name: string;
      /**
       * @description Selected embedding model.
       * @enum {string}
       */
      model: "jina-v2" | "clip";
      /** @description Document IDs to delete. */
      ids: string[];
    };
    /** QueryVectorStoreIn */
    QueryVectorStoreIn: {
      /** @description Vector store to query against. */
      collection_name: string;
      /**
       * @description Selected embedding model.
       * @enum {string}
       */
      model: "jina-v2" | "clip";
      /** @description Texts to embed and use for the query. */
      query_strings?: string[];
      /** @description Image URIs to embed and use for the query. */
      query_image_uris?: string[];
      /** @description Vectors to use for the query. */
      query_vectors?: number[][];
      /** @description Document IDs to use for the query. */
      query_ids?: string[];
      /**
       * @description Number of results to return.
       * @default 10
       */
      top_k?: number;
      /**
       * @description The size of the dynamic candidate list for searching the index graph.
       * @default 40
       */
      ef_search?: number;
      /**
       * @description The number of leaves in the index tree to search.
       * @default 40
       */
      num_leaves_to_search?: number;
      /**
       * @description Include the values of the vectors in the response.
       * @default false
       */
      include_values?: boolean;
      /**
       * @description Include the metadata of the vectors in the response.
       * @default false
       */
      include_metadata?: boolean;
      /** @description Filter metadata by key-value pairs. */
      filters?: {
        [key: string]: unknown;
      };
    };
    /** VectorStoreQueryResult */
    VectorStoreQueryResult: {
      /** @description Document ID. */
      id: string;
      /**
       * Format: float
       * @description Similarity score.
       */
      distance: number;
      /** @description Embedding vector. */
      vector?: number[];
      /** @description Document metadata. */
      metadata?: {
        [key: string]: unknown;
      };
    };
    /** QueryVectorStoreOut */
    QueryVectorStoreOut: {
      /** @description Query results. */
      results: {
        /** @description Document ID. */
        id: string;
        /**
         * Format: float
         * @description Similarity score.
         */
        distance: number;
        /** @description Embedding vector. */
        vector?: number[];
        /** @description Document metadata. */
        metadata?: {
          [key: string]: unknown;
        };
      }[][];
      /** @description Vector store name. */
      collection_name?: string;
      /**
       * @description Selected embedding model.
       * @enum {string}
       */
      model?: "jina-v2" | "clip";
    };
    /** SplitDocumentIn */
    SplitDocumentIn: {
      /** @description URI of the document. */
      uri: string;
      /** @description Document ID. */
      doc_id?: string;
      /** @description Document metadata. */
      metadata?: {
        [key: string]: unknown;
      };
      /** @description Maximum number of units per chunk. Defaults to 1024 tokens for text or 40 lines for code. */
      chunk_size?: number;
      /** @description Number of units to overlap between chunks. Defaults to 200 tokens for text or 15 lines for code. */
      chunk_overlap?: number;
    };
    /** SplitDocumentOut */
    SplitDocumentOut: {
      /** @description Document chunks */
      items: {
        /** @description Text to embed. */
        text: string;
        /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
        metadata?: {
          [key: string]: unknown;
        };
        /** @description Vector store document ID. Ignored if `collection_name` is unset. */
        doc_id?: string;
      }[];
    };
  };
  responses: never;
  parameters: never;
  requestBodies: never;
  headers: never;
  pathItems: never;
}

export type $defs = Record<string, never>;

export type external = Record<string, never>;

export interface operations {
  /**
   * Experimental
   * @description Experimental node.
   */
  Experimental: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "name": "some_name",
         *   "args": {
         *     "foo": "bar"
         *   }
         * }
         */
        "application/json": {
          /** @description Identifier. */
          name: string;
          /** @description Arguments. */
          args: {
            [key: string]: unknown;
          };
          /**
           * @description Timeout in seconds.
           * @default 60
           */
          timeout?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Response. */
            output: {
              [key: string]: unknown;
            };
          };
        };
      };
    };
  };
  /**
   * Box
   * @description Combine multiple values into a single output.
   */
  Box: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "value": {
         *     "a": "b",
         *     "c": {
         *       "d": [
         *         1,
         *         2,
         *         3
         *       ]
         *     }
         *   }
         * }
         */
        "application/json": {
          /** @description Values to box. */
          value: unknown;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description The evaluated result. */
            value: unknown;
          };
        };
      };
    };
  };
  /**
   * If
   * @description Return one of two options based on a condition.
   */
  If: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "condition": true,
         *   "value_if_true": "yes",
         *   "value_if_false": "no"
         * }
         */
        "application/json": {
          /** @description Condition. */
          condition: boolean;
          /** @description Result when condition is true. */
          value_if_true: unknown;
          /** @description Result when condition is false. */
          value_if_false?: unknown;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Result. Null if `value_if_false` is not provided and `condition` is false. */
            result: unknown;
          };
        };
      };
    };
  };
  /**
   * RunPython
   * @description Run code using a Python interpreter.
   */
  RunPython: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "pkl_function": "g2UjA5fX2t3ZGVmYXVsdHNfX5ROjAxfX2RlZmF1bHRzX1+UTowKX19tb2R1bGVfX5SMCF9fbWFpbl9flIwHX19kb2NfX5ROjAtfX2Nsb3N1cmVfX5ROjBdfY2xvdWRwaWNrbGVfc3VibW9kdWxlc5RdlIwLX19nbG9iYWxzX1+UfZR1hpSGUjAu",
         *   "kwargs": {},
         *   "pip_install": [
         *     "numpy"
         *   ]
         * }
         */
        "application/json": {
          /** @description Pickled function. */
          pkl_function?: string;
          /** @description Keyword arguments to your function. */
          kwargs: {
            [key: string]: unknown;
          };
          /** @description Python version. */
          python_version?: string;
          /** @description Python packages to install. You must import them in your code. */
          pip_install?: string[];
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Return value of your function. */
            output?: unknown;
            /** @description Pickled return value. */
            pkl_output?: string;
            /** @description Everything printed to stdout while running your code. */
            stdout: string;
            /** @description Contents of stderr if your code did not run successfully. */
            stderr: string;
          };
        };
      };
    };
  };
  /**
   * ComputeText
   * @description Compute text using a language model.
   */
  ComputeText: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "Who is Don Quixote?",
         *   "temperature": 0.4,
         *   "max_tokens": 800
         * }
         */
        "application/json": {
          /** @description Input prompt. */
          prompt: string;
          /** @description Image prompts. */
          image_uris?: string[];
          /**
           * Format: float
           * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
           * @default 0.4
           */
          temperature?: number;
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
          /**
           * @description Selected model. `Firellava13B` is automatically selected when `image_uris` is provided.
           * @default Llama3Instruct8B
           * @enum {string}
           */
          model?:
            | "Mistral7BInstruct"
            | "Mixtral8x7BInstruct"
            | "Llama3Instruct8B"
            | "Llama3Instruct70B"
            | "Llama3Instruct405B"
            | "Firellava13B"
            | "gpt-4o"
            | "gpt-4o-mini"
            | "claude-3-5-sonnet-20240620";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Text response. */
            text: string;
          };
        };
      };
    };
  };
  /**
   * MultiComputeText
   * @description Generate multiple text choices using a language model.
   */
  MultiComputeText: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "Who is Don Quixote?",
         *   "num_choices": 2,
         *   "max_tokens": 800
         * }
         */
        "application/json": {
          /** @description Input prompt. */
          prompt: string;
          /**
           * @description Number of choices to generate.
           * @default 1
           */
          num_choices: number;
          /**
           * Format: float
           * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
           * @default 0.4
           */
          temperature?: number;
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
          /**
           * @description Selected model.
           * @default Llama3Instruct8B
           * @enum {string}
           */
          model?:
            | "Mistral7BInstruct"
            | "Mixtral8x7BInstruct"
            | "Llama3Instruct8B"
            | "Llama3Instruct70B";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Response choices. */
            choices: {
              /** @description Text response. */
              text: string;
            }[];
          };
        };
      };
    };
  };
  /**
   * BatchComputeText
   * @description Compute text for multiple prompts in batch using a language model.
   */
  BatchComputeText: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompts": [
         *     "Who is Don Quixote?",
         *     "Who is Sancho Panza?"
         *   ],
         *   "max_tokens": 800
         * }
         */
        "application/json": {
          /** @description Batch input prompts. */
          prompts: string[];
          /**
           * Format: float
           * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
           * @default 0.4
           */
          temperature?: number;
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
          /**
           * @description Selected model.
           * @default Llama3Instruct8B
           * @enum {string}
           */
          model?: "Mistral7BInstruct" | "Llama3Instruct8B";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Batch outputs. */
            outputs: {
              /** @description Text response. */
              text: string;
            }[];
          };
        };
      };
    };
  };
  /**
   * BatchComputeJSON
   * @description Compute JSON for multiple prompts in batch using a language model.
   */
  BatchComputeJSON: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompts": [
         *     "Who is Don Quixote?",
         *     "Who is Sancho Panza?"
         *   ],
         *   "max_tokens": 800,
         *   "json_schema": {
         *     "type": "object",
         *     "properties": {
         *       "name": {
         *         "type": "string",
         *         "description": "The name of the character."
         *       },
         *       "bio": {
         *         "type": "string",
         *         "description": "Concise biography of the character."
         *       }
         *     }
         *   }
         * }
         */
        "application/json": {
          /** @description Batch input prompts. */
          prompts: string[];
          /** @description JSON schema to guide `json_object` response. */
          json_schema: {
            [key: string]: unknown;
          };
          /**
           * Format: float
           * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
           * @default 0.4
           */
          temperature?: number;
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
          /**
           * @description Selected model.
           * @default Llama3Instruct8B
           * @enum {string}
           */
          model?: "Mistral7BInstruct" | "Llama3Instruct8B";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Batch outputs. */
            outputs: {
              /** @description JSON response. */
              json_object?: {
                [key: string]: unknown;
              };
              /** @description If the model output could not be parsed to JSON, this is the raw text output. */
              text?: string;
            }[];
          };
        };
      };
    };
  };
  /**
   * ComputeJSON
   * @description Compute JSON using a language model.
   */
  ComputeJSON: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "Who wrote Don Quixote?",
         *   "json_schema": {
         *     "type": "object",
         *     "properties": {
         *       "name": {
         *         "type": "string",
         *         "description": "The name of the author."
         *       },
         *       "bio": {
         *         "type": "string",
         *         "description": "Concise biography of the author."
         *       }
         *     }
         *   },
         *   "temperature": 0.4,
         *   "max_tokens": 800
         * }
         */
        "application/json": {
          /** @description Input prompt. */
          prompt: string;
          /** @description JSON schema to guide `json_object` response. */
          json_schema: {
            [key: string]: unknown;
          };
          /**
           * Format: float
           * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
           * @default 0.4
           */
          temperature?: number;
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
          /**
           * @description Selected model.
           * @default Llama3Instruct8B
           * @enum {string}
           */
          model?:
            | "Mistral7BInstruct"
            | "Mixtral8x7BInstruct"
            | "Llama3Instruct8B";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description JSON response. */
            json_object?: {
              [key: string]: unknown;
            };
            /** @description If the model output could not be parsed to JSON, this is the raw text output. */
            text?: string;
          };
        };
      };
    };
  };
  /**
   * MultiComputeJSON
   * @description Compute multiple JSON choices using a language model.
   */
  MultiComputeJSON: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "Who wrote Don Quixote?",
         *   "json_schema": {
         *     "type": "object",
         *     "properties": {
         *       "name": {
         *         "type": "string",
         *         "description": "The name of the author."
         *       },
         *       "bio": {
         *         "type": "string",
         *         "description": "Concise biography of the author."
         *       }
         *     }
         *   },
         *   "num_choices": 2,
         *   "temperature": 0.4,
         *   "max_tokens": 800
         * }
         */
        "application/json": {
          /** @description Input prompt. */
          prompt: string;
          /** @description JSON schema to guide `json_object` response. */
          json_schema: {
            [key: string]: unknown;
          };
          /**
           * @description Number of choices to generate.
           * @default 2
           */
          num_choices: number;
          /**
           * Format: float
           * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
           * @default 0.4
           */
          temperature?: number;
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
          /**
           * @description Selected model.
           * @default Llama3Instruct8B
           * @enum {string}
           */
          model?:
            | "Mistral7BInstruct"
            | "Mixtral8x7BInstruct"
            | "Llama3Instruct8B";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Response choices. */
            choices: {
              /** @description JSON response. */
              json_object?: {
                [key: string]: unknown;
              };
              /** @description If the model output could not be parsed to JSON, this is the raw text output. */
              text?: string;
            }[];
          };
        };
      };
    };
  };
  /**
   * Mistral7BInstruct
   * @description Compute text using [Mistral 7B Instruct](https://mistral.ai/news/announcing-mistral-7b).
   */
  Mistral7BInstruct: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "Who is Don Quixote?",
         *   "num_choices": 2,
         *   "temperature": 0.4,
         *   "max_tokens": 800
         * }
         */
        "application/json": {
          /** @description Input prompt. */
          prompt: string;
          /** @description System prompt. */
          system_prompt?: string;
          /**
           * @description Number of choices to generate.
           * @default 1
           */
          num_choices?: number;
          /** @description JSON schema to guide response. */
          json_schema?: {
            [key: string]: unknown;
          };
          /**
           * Format: float
           * @description Higher values make the output more random, lower values make the output more deterministic.
           */
          temperature?: number;
          /**
           * Format: float
           * @description Higher values decrease the likelihood of repeating previous tokens.
           * @default 0
           */
          frequency_penalty?: number;
          /**
           * Format: float
           * @description Higher values decrease the likelihood of repeated sequences.
           * @default 1
           */
          repetition_penalty?: number;
          /**
           * Format: float
           * @description Higher values increase the likelihood of new topics appearing.
           * @default 1.1
           */
          presence_penalty?: number;
          /**
           * Format: float
           * @description Probability below which less likely tokens are filtered out.
           * @default 0.95
           */
          top_p?: number;
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Response choices. */
            choices: {
              /** @description Text response, if `json_schema` was not provided. */
              text?: string;
              /** @description JSON response, if `json_schema` was provided. */
              json_object?: {
                [key: string]: unknown;
              };
            }[];
          };
        };
      };
    };
  };
  /**
   * Mixtral8x7BInstruct
   * @description Compute text using instruct-tuned [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts/).
   */
  Mixtral8x7BInstruct: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "Who is Don Quixote?",
         *   "num_choices": 2,
         *   "temperature": 0.4,
         *   "max_tokens": 800
         * }
         */
        "application/json": {
          /** @description Input prompt. */
          prompt: string;
          /** @description System prompt. */
          system_prompt?: string;
          /**
           * @description Number of choices to generate.
           * @default 1
           */
          num_choices?: number;
          /** @description JSON schema to guide response. */
          json_schema?: {
            [key: string]: unknown;
          };
          /**
           * Format: float
           * @description Higher values make the output more random, lower values make the output more deterministic.
           */
          temperature?: number;
          /**
           * Format: float
           * @description Higher values decrease the likelihood of repeating previous tokens.
           * @default 0
           */
          frequency_penalty?: number;
          /**
           * Format: float
           * @description Higher values decrease the likelihood of repeated sequences.
           * @default 1
           */
          repetition_penalty?: number;
          /**
           * Format: float
           * @description Higher values increase the likelihood of new topics appearing.
           * @default 1.1
           */
          presence_penalty?: number;
          /**
           * Format: float
           * @description Probability below which less likely tokens are filtered out.
           * @default 0.95
           */
          top_p?: number;
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Response choices. */
            choices: {
              /** @description Text response, if `json_schema` was not provided. */
              text?: string;
              /** @description JSON response, if `json_schema` was provided. */
              json_object?: {
                [key: string]: unknown;
              };
            }[];
          };
        };
      };
    };
  };
  /**
   * Llama3Instruct8B
   * @description Compute text using instruct-tuned [Llama 3 8B](https://llama.meta.com/llama3/).
   */
  Llama3Instruct8B: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "Who is Don Quixote?",
         *   "num_choices": 2,
         *   "temperature": 0.4,
         *   "max_tokens": 800
         * }
         */
        "application/json": {
          /** @description Input prompt. */
          prompt: string;
          /** @description System prompt. */
          system_prompt?: string;
          /**
           * @description Number of choices to generate.
           * @default 1
           */
          num_choices?: number;
          /**
           * Format: float
           * @description Higher values make the output more random, lower values make the output more deterministic.
           */
          temperature?: number;
          /**
           * Format: float
           * @description Higher values decrease the likelihood of repeating previous tokens.
           * @default 0
           */
          frequency_penalty?: number;
          /**
           * Format: float
           * @description Higher values decrease the likelihood of repeated sequences.
           * @default 1
           */
          repetition_penalty?: number;
          /**
           * Format: float
           * @description Higher values increase the likelihood of new topics appearing.
           * @default 1.1
           */
          presence_penalty?: number;
          /**
           * Format: float
           * @description Probability below which less likely tokens are filtered out.
           * @default 0.95
           */
          top_p?: number;
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
          /** @description JSON schema to guide response. */
          json_schema?: {
            [key: string]: unknown;
          };
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Response choices. */
            choices: {
              /** @description Text response. */
              text?: string;
              /** @description JSON response, if `json_schema` was provided. */
              json_object?: {
                [key: string]: unknown;
              };
            }[];
          };
        };
      };
    };
  };
  /**
   * Llama3Instruct70B
   * @description Compute text using instruct-tuned [Llama 3 70B](https://llama.meta.com/llama3/).
   */
  Llama3Instruct70B: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "Who is Don Quixote?",
         *   "num_choices": 2,
         *   "temperature": 0.4,
         *   "max_tokens": 800
         * }
         */
        "application/json": {
          /** @description Input prompt. */
          prompt: string;
          /** @description System prompt. */
          system_prompt?: string;
          /**
           * @description Number of choices to generate.
           * @default 1
           */
          num_choices?: number;
          /**
           * Format: float
           * @description Higher values make the output more random, lower values make the output more deterministic.
           */
          temperature?: number;
          /**
           * Format: float
           * @description Higher values decrease the likelihood of repeating previous tokens.
           * @default 0
           */
          frequency_penalty?: number;
          /**
           * Format: float
           * @description Higher values decrease the likelihood of repeated sequences.
           * @default 1
           */
          repetition_penalty?: number;
          /**
           * Format: float
           * @description Higher values increase the likelihood of new topics appearing.
           * @default 1.1
           */
          presence_penalty?: number;
          /**
           * Format: float
           * @description Probability below which less likely tokens are filtered out.
           * @default 0.95
           */
          top_p?: number;
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Response choices. */
            choices: {
              /** @description Text response. */
              text?: string;
            }[];
          };
        };
      };
    };
  };
  /**
   * Firellava13B
   * @description Compute text with image input using [FireLLaVA 13B](https://fireworks.ai/blog/firellava-the-first-commercially-permissive-oss-llava-model).
   */
  Firellava13B: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "what are these paintings of and who made them?",
         *   "image_uris": [
         *     "https://media.substrate.run/docs-fuji-red.jpg",
         *     "https://media.substrate.run/docs-fuji-blue.jpg"
         *   ]
         * }
         */
        "application/json": {
          /** @description Text prompt. */
          prompt: string;
          /** @description Image prompts. */
          image_uris: string[];
          /** @description Maximum number of tokens to generate. */
          max_tokens?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Text response. */
            text: string;
          };
        };
      };
    };
  };
  /**
   * GenerateImage
   * @description Generate an image.
   */
  GenerateImage: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "hokusai futuristic supercell spiral cloud with glowing core over turbulent ocean",
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Text prompt. */
          prompt: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
          };
        };
      };
    };
  };
  /**
   * MultiGenerateImage
   * @description Generate multiple images.
   */
  MultiGenerateImage: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "hokusai futuristic supercell spiral cloud with glowing core over turbulent ocean",
         *   "num_images": 2,
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Text prompt. */
          prompt: string;
          /**
           * @description Number of images to generate.
           * @default 2
           */
          num_images: number;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated images. */
            outputs: {
              /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
              image_uri: string;
            }[];
          };
        };
      };
    };
  };
  /**
   * InpaintImage
   * @description Edit an image using image generation inside part of the image or the full image.
   */
  InpaintImage: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "image_uri": "https://media.substrate.run/docs-klimt-park.jpg",
         *   "mask_image_uri": "https://media.substrate.run/spiral-logo.jpeg",
         *   "prompt": "large tropical colorful bright anime birds in a dark jungle full of vines, high resolution",
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Original image. */
          image_uri: string;
          /** @description Text prompt. */
          prompt: string;
          /** @description Mask image that controls which pixels are inpainted. If unset, the entire image is edited (image-to-image). */
          mask_image_uri?: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
          };
        };
      };
    };
  };
  /**
   * MultiInpaintImage
   * @description Edit multiple images using image generation.
   */
  MultiInpaintImage: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "image_uri": "https://media.substrate.run/docs-klimt-park.jpg",
         *   "mask_image_uri": "https://media.substrate.run/spiral-logo.jpeg",
         *   "prompt": "large tropical colorful bright anime birds in a dark jungle full of vines, high resolution",
         *   "num_images": 2,
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Original image. */
          image_uri: string;
          /** @description Text prompt. */
          prompt: string;
          /** @description Mask image that controls which pixels are edited (inpainting). If unset, the entire image is edited (image-to-image). */
          mask_image_uri?: string;
          /**
           * @description Number of images to generate.
           * @default 2
           */
          num_images: number;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated images. */
            outputs: {
              /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
              image_uri: string;
            }[];
          };
        };
      };
    };
  };
  /**
   * StableDiffusionXLLightning
   * @description Generate an image using [Stable Diffusion XL Lightning](https://arxiv.org/abs/2402.13929).
   */
  StableDiffusionXLLightning: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "hokusai futuristic supercell spiral cloud with glowing core over turbulent ocean",
         *   "negative_prompt": "night, moon",
         *   "num_images": 2,
         *   "seeds": [
         *     330699,
         *     136464
         *   ],
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Text prompt. */
          prompt: string;
          /** @description Negative input prompt. */
          negative_prompt?: string;
          /**
           * @description Number of images to generate.
           * @default 1
           */
          num_images?: number;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
          /**
           * @description Height of output image, in pixels.
           * @default 1024
           */
          height?: number;
          /**
           * @description Width of output image, in pixels.
           * @default 1024
           */
          width?: number;
          /** @description Seeds for deterministic generation. Default is a random seed. */
          seeds?: number[];
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated images. */
            outputs: {
              /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
              image_uri: string;
              /** @description The random noise seed used for generation. */
              seed: number;
            }[];
          };
        };
      };
    };
  };
  /**
   * StableDiffusionXLInpaint
   * @description Edit an image using [Stable Diffusion XL](https://arxiv.org/abs/2307.01952). Supports inpainting (edit part of the image with a mask) and image-to-image (edit the full image).
   */
  StableDiffusionXLInpaint: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "image_uri": "https://media.substrate.run/docs-klimt-park.jpg",
         *   "mask_image_uri": "https://media.substrate.run/spiral-logo.jpeg",
         *   "prompt": "large tropical colorful bright birds in a jungle, high resolution oil painting",
         *   "negative_prompt": "dark, cartoon, anime",
         *   "strength": 0.8,
         *   "num_images": 2,
         *   "store": "hosted",
         *   "seeds": [
         *     1607280,
         *     1720395
         *   ]
         * }
         */
        "application/json": {
          /** @description Original image. */
          image_uri: string;
          /** @description Text prompt. */
          prompt: string;
          /** @description Mask image that controls which pixels are edited (inpainting). If unset, the entire image is edited (image-to-image). */
          mask_image_uri?: string;
          /**
           * @description Number of images to generate.
           * @default 1
           */
          num_images: number;
          /**
           * @description Resolution of the output image, in pixels.
           * @default 1024
           */
          output_resolution?: number;
          /** @description Negative input prompt. */
          negative_prompt?: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
          /**
           * Format: float
           * @description Controls the strength of the generation process.
           * @default 0.8
           */
          strength?: number;
          /** @description Random noise seeds. Default is random seeds for each generation. */
          seeds?: number[];
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated images. */
            outputs: {
              /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
              image_uri: string;
              /** @description The random noise seed used for generation. */
              seed: number;
            }[];
          };
        };
      };
    };
  };
  /**
   * StableDiffusionXLControlNet
   * @description Generate an image with generation structured by an input image, using Stable Diffusion XL with [ControlNet](https://arxiv.org/abs/2302.05543).
   */
  StableDiffusionXLControlNet: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "image_uri": "https://media.substrate.run/spiral-logo.jpeg",
         *   "prompt": "the futuristic solarpunk city of atlantis at sunset, cinematic bokeh HD",
         *   "control_method": "illusion",
         *   "conditioning_scale": 1,
         *   "strength": 1,
         *   "store": "hosted",
         *   "num_images": 2,
         *   "seeds": [
         *     1607226,
         *     1720395
         *   ]
         * }
         */
        "application/json": {
          /** @description Input image. */
          image_uri: string;
          /**
           * @description Strategy to control generation using the input image.
           * @enum {string}
           */
          control_method: "edge" | "depth" | "illusion" | "tile";
          /** @description Text prompt. */
          prompt: string;
          /**
           * @description Number of images to generate.
           * @default 1
           */
          num_images: number;
          /**
           * @description Resolution of the output image, in pixels.
           * @default 1024
           */
          output_resolution?: number;
          /** @description Negative input prompt. */
          negative_prompt?: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
          /**
           * Format: float
           * @description Controls the influence of the input image on the generated output.
           * @default 0.5
           */
          conditioning_scale?: number;
          /**
           * Format: float
           * @description Controls how much to transform the input image.
           * @default 0.5
           */
          strength?: number;
          /** @description Random noise seeds. Default is random seeds for each generation. */
          seeds?: number[];
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated images. */
            outputs: {
              /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
              image_uri: string;
              /** @description The random noise seed used for generation. */
              seed: number;
            }[];
          };
        };
      };
    };
  };
  /**
   * StableVideoDiffusion
   * @description Generates a video using a still image as conditioning frame.
   */
  StableVideoDiffusion: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "image_uri": "https://media.substrate.run/apple-forest.jpeg",
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Original image. */
          image_uri: string;
          /** @description Use "hosted" to return a video URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the video data will be returned as a base64-encoded string. */
          store?: string;
          /**
           * @description Output video format.
           * @default gif
           * @enum {string}
           */
          output_format?: "gif" | "webp" | "mp4" | "frames";
          /** @description Seed for deterministic generation. Default is a random seed. */
          seed?: number;
          /**
           * @description Frames per second of the generated video. Ignored if output format is `frames`.
           * @default 7
           */
          fps?: number;
          /**
           * @description The motion bucket id to use for the generated video. This can be used to control the motion of the generated video. Increasing the motion bucket id increases the motion of the generated video.
           * @default 180
           */
          motion_bucket_id?: number;
          /**
           * Format: float
           * @description The amount of noise added to the conditioning image. The higher the values the less the video resembles the conditioning image. Increasing this value also increases the motion of the generated video.
           * @default 0.1
           */
          noise?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated video. */
            video_uri?: string;
            /** @description Generated frames. */
            frame_uris?: string[];
          };
        };
      };
    };
  };
  /**
   * InterpolateFrames
   * @description Generates a interpolation frames between each adjacent frames.
   */
  InterpolateFrames: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "frame_uris": [
         *     "https://media.substrate.run/apple-forest2.jpeg",
         *     "https://media.substrate.run/apple-forest3.jpeg"
         *   ],
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Frames. */
          frame_uris: string[];
          /** @description Use "hosted" to return a video URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the video data will be returned as a base64-encoded string. */
          store?: string;
          /**
           * @description Output video format.
           * @default gif
           * @enum {string}
           */
          output_format?: "gif" | "webp" | "mp4" | "frames";
          /**
           * @description Frames per second of the generated video. Ignored if output format is `frames`.
           * @default 7
           */
          fps?: number;
          /**
           * @description Number of interpolation steps. Each step adds an interpolated frame between adjacent frames. For example, 2 steps over 2 frames produces 5 frames.
           * @default 2
           */
          num_steps?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated video. */
            video_uri?: string;
            /** @description Output frames. */
            frame_uris?: string[];
          };
        };
      };
    };
  };
  /**
   * TranscribeSpeech
   * @description Transcribe speech in an audio or video file.
   */
  TranscribeSpeech: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "audio_uri": "https://media.substrate.run/dfw-clip.m4a",
         *   "prompt": "David Foster Wallace interviewed about US culture, and Infinite Jest",
         *   "segment": true,
         *   "align": true,
         *   "diarize": true,
         *   "suggest_chapters": true
         * }
         */
        "application/json": {
          /** @description Input audio. */
          audio_uri: string;
          /** @description Prompt to guide model on the content and context of input audio. */
          prompt?: string;
          /**
           * @description Language of input audio in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format.
           * @default en
           */
          language?: string;
          /**
           * @description Segment the text into sentences with approximate timestamps.
           * @default false
           */
          segment?: boolean;
          /**
           * @description Align transcription to produce more accurate sentence-level timestamps and word-level timestamps. An array of word segments will be included in each sentence segment.
           * @default false
           */
          align?: boolean;
          /**
           * @description Identify speakers for each segment. Speaker IDs will be included in each segment.
           * @default false
           */
          diarize?: boolean;
          /**
           * @description Suggest automatic chapter markers.
           * @default false
           */
          suggest_chapters?: boolean;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Transcribed text. */
            text: string;
            /** @description Transcribed segments, if `segment` is enabled. */
            segments?: {
              /** @description Text of segment. */
              text: string;
              /**
               * Format: float
               * @description Start time of segment, in seconds.
               */
              start: number;
              /**
               * Format: float
               * @description End time of segment, in seconds.
               */
              end: number;
              /** @description ID of speaker, if `diarize` is enabled. */
              speaker?: string;
              /** @description Aligned words, if `align` is enabled. */
              words?: {
                /** @description Text of word. */
                word: string;
                /**
                 * Format: float
                 * @description Start time of word, in seconds.
                 */
                start?: number;
                /**
                 * Format: float
                 * @description End time of word, in seconds.
                 */
                end?: number;
                /** @description ID of speaker, if `diarize` is enabled. */
                speaker?: string;
              }[];
            }[];
            /** @description Chapter markers, if `suggest_chapters` is enabled. */
            chapters?: {
              /** @description Chapter title. */
              title: string;
              /**
               * Format: float
               * @description Start time of chapter, in seconds.
               */
              start: number;
            }[];
          };
        };
      };
    };
  };
  /**
   * GenerateSpeech
   * @description Generate speech from text.
   */
  GenerateSpeech: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "text": "Substrate: an underlying substance or layer.",
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Input text. */
          text: string;
          /** @description Use "hosted" to return an audio URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the audio data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded WAV audio bytes, or a hosted audio url if `store` is provided. */
            audio_uri: string;
          };
        };
      };
    };
  };
  /**
   * RemoveBackground
   * @description Remove the background from an image and return the foreground segment as a cut-out or a mask.
   */
  RemoveBackground: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "image_uri": "https://media.substrate.run/apple-forest.jpeg",
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Input image. */
          image_uri: string;
          /**
           * @description Return a mask image instead of the original content.
           * @default false
           */
          return_mask?: boolean;
          /**
           * @description Invert the mask image. Only takes effect if `return_mask` is true.
           * @default false
           */
          invert_mask?: boolean;
          /** @description Hex value background color. Transparent if unset. */
          background_color?: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
          };
        };
      };
    };
  };
  /**
   * EraseImage
   * @description Erase the masked part of an image, e.g. to remove an object by inpainting.
   */
  EraseImage: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "image_uri": "https://media.substrate.run/apple-forest.jpeg",
         *   "mask_image_uri": "https://media.substrate.run/apple-forest-mask.jpeg",
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Input image. */
          image_uri: string;
          /** @description Mask image that controls which pixels are inpainted. */
          mask_image_uri: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
          };
        };
      };
    };
  };
  /**
   * UpscaleImage
   * @description Upscale an image using image generation.
   */
  UpscaleImage: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "prompt": "high resolution detailed spiral shell",
         *   "image_uri": "https://media.substrate.run/docs-shell-emoji.jpg",
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Prompt to guide model on the content of image to upscale. */
          prompt?: string;
          /** @description Input image. */
          image_uri: string;
          /**
           * @description Resolution of the output image, in pixels.
           * @default 1024
           */
          output_resolution?: number;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
          };
        };
      };
    };
  };
  /**
   * SegmentUnderPoint
   * @description Segment an image under a point and return the segment.
   */
  SegmentUnderPoint: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "image_uri": "https://media.substrate.run/docs-vg-bedroom.jpg",
         *   "point": {
         *     "x": 189,
         *     "y": 537
         *   },
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Input image. */
          image_uri: string;
          /** Point */
          point: {
            /** @description X position. */
            x: number;
            /** @description Y position. */
            y: number;
          };
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Detected segments in 'mask image' format. Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            mask_image_uri: string;
          };
        };
      };
    };
  };
  /**
   * SegmentAnything
   * @description Segment an image using [SegmentAnything](https://github.com/facebookresearch/segment-anything).
   */
  SegmentAnything: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "image_uri": "https://media.substrate.run/docs-vg-bedroom.jpg",
         *   "point_prompts": [
         *     {
         *       "x": 189,
         *       "y": 537
         *     }
         *   ],
         *   "store": "hosted"
         * }
         */
        "application/json": {
          /** @description Input image. */
          image_uri: string;
          /** @description Point prompts, to detect a segment under the point. One of `point_prompts` or `box_prompts` must be set. */
          point_prompts?: {
            /** @description X position. */
            x: number;
            /** @description Y position. */
            y: number;
          }[];
          /** @description Box prompts, to detect a segment within the bounding box. One of `point_prompts` or `box_prompts` must be set. */
          box_prompts?: {
            /**
             * Format: float
             * @description Top left corner x.
             */
            x1: number;
            /**
             * Format: float
             * @description Top left corner y.
             */
            y1: number;
            /**
             * Format: float
             * @description Bottom right corner x.
             */
            x2: number;
            /**
             * Format: float
             * @description Bottom right corner y.
             */
            y2: number;
          }[];
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](https://docs.substrate.run/reference/external-files). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Detected segments in 'mask image' format. Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            mask_image_uri: string;
          };
        };
      };
    };
  };
  /**
   * SplitDocument
   * @description Split document into text segments.
   */
  SplitDocument: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "doc_id": "example_pdf",
         *   "uri": "https://arxiv.org/pdf/2405.07945",
         *   "metadata": {
         *     "title": "GRASS II: Simulations of Potential Granulation Noise Mitigation Methods"
         *   }
         * }
         */
        "application/json": {
          /** @description URI of the document. */
          uri: string;
          /** @description Document ID. */
          doc_id?: string;
          /** @description Document metadata. */
          metadata?: {
            [key: string]: unknown;
          };
          /** @description Maximum number of units per chunk. Defaults to 1024 tokens for text or 40 lines for code. */
          chunk_size?: number;
          /** @description Number of units to overlap between chunks. Defaults to 200 tokens for text or 15 lines for code. */
          chunk_overlap?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Document chunks */
            items: {
              /** @description Text to embed. */
              text: string;
              /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
              metadata?: {
                [key: string]: unknown;
              };
              /** @description Vector store document ID. Ignored if `collection_name` is unset. */
              doc_id?: string;
            }[];
          };
        };
      };
    };
  };
  /**
   * EmbedText
   * @description Generate embedding for a text document.
   */
  EmbedText: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "text": "Argon is the third most abundant gas in Earth's atmosphere, at 0.934% (9340 ppmv). It is more than twice as abundant as water vapor.",
         *   "model": "jina-v2",
         *   "collection_name": "smoke_tests",
         *   "metadata": {
         *     "group": "18"
         *   },
         *   "embedded_metadata_keys": [
         *     "group"
         *   ]
         * }
         */
        "application/json": {
          /** @description Text to embed. */
          text: string;
          /** @description Vector store name. */
          collection_name?: string;
          /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
          metadata?: {
            [key: string]: unknown;
          };
          /** @description Choose keys from `metadata` to embed with text. */
          embedded_metadata_keys?: string[];
          /** @description Vector store document ID. Ignored if `store` is unset. */
          doc_id?: string;
          /**
           * @description Selected embedding model.
           * @default jina-v2
           * @enum {string}
           */
          model?: "jina-v2" | "clip";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** Embedding */
            embedding: {
              /** @description Embedding vector. */
              vector: number[];
              /** @description Vector store document ID. */
              doc_id?: string;
              /** @description Vector store document metadata. */
              metadata?: {
                [key: string]: unknown;
              };
            };
          };
        };
      };
    };
  };
  /**
   * MultiEmbedText
   * @description Generate embeddings for multiple text documents.
   */
  MultiEmbedText: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "model": "jina-v2",
         *   "items": [
         *     {
         *       "text": "Osmium is the densest naturally occurring element. When experimentally measured using X-ray crystallography, it has a density of 22.59 g/cm3. Manufacturers use its alloys with platinum, iridium, and other platinum-group metals to make fountain pen nib tipping, electrical contacts, and in other applications that require extreme durability and hardness.",
         *       "metadata": {
         *         "group": "8"
         *       }
         *     },
         *     {
         *       "text": "Despite its abundant presence in the universe and Solar Systemranking fifth in cosmic abundance following hydrogen, helium, oxygen, and carbonneon is comparatively scarce on Earth.",
         *       "metadata": {
         *         "group": "18"
         *       }
         *     }
         *   ],
         *   "collection_name": "smoke_tests",
         *   "embedded_metadata_keys": [
         *     "group"
         *   ]
         * }
         */
        "application/json": {
          /** @description Items to embed. */
          items: {
            /** @description Text to embed. */
            text: string;
            /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
            metadata?: {
              [key: string]: unknown;
            };
            /** @description Vector store document ID. Ignored if `collection_name` is unset. */
            doc_id?: string;
          }[];
          /** @description Vector store name. */
          collection_name?: string;
          /** @description Choose keys from `metadata` to embed with text. */
          embedded_metadata_keys?: string[];
          /**
           * @description Selected embedding model.
           * @default jina-v2
           * @enum {string}
           */
          model?: "jina-v2" | "clip";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated embeddings. */
            embeddings: {
              /** @description Embedding vector. */
              vector: number[];
              /** @description Vector store document ID. */
              doc_id?: string;
              /** @description Vector store document metadata. */
              metadata?: {
                [key: string]: unknown;
              };
            }[];
          };
        };
      };
    };
  };
  /**
   * EmbedImage
   * @description Generate embedding for an image.
   */
  EmbedImage: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "image_uri": "https://media.substrate.run/docs-fuji-red.jpg",
         *   "collection_name": "smoke_tests"
         * }
         */
        "application/json": {
          /** @description Image to embed. */
          image_uri: string;
          /** @description Vector store name. */
          collection_name?: string;
          /** @description Vector store document ID. Ignored if `collection_name` is unset. */
          doc_id?: string;
          /**
           * @description Selected embedding model.
           * @default clip
           * @enum {string}
           */
          model?: "clip";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** Embedding */
            embedding: {
              /** @description Embedding vector. */
              vector: number[];
              /** @description Vector store document ID. */
              doc_id?: string;
              /** @description Vector store document metadata. */
              metadata?: {
                [key: string]: unknown;
              };
            };
          };
        };
      };
    };
  };
  /**
   * MultiEmbedImage
   * @description Generate embeddings for multiple images.
   */
  MultiEmbedImage: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "items": [
         *     {
         *       "image_uri": "https://media.substrate.run/docs-fuji-red.jpg"
         *     },
         *     {
         *       "image_uri": "https://media.substrate.run/docs-fuji-blue.jpg"
         *     }
         *   ],
         *   "collection_name": "smoke_tests"
         * }
         */
        "application/json": {
          /** @description Items to embed. */
          items: {
            /** @description Image to embed. */
            image_uri: string;
            /** @description Vector store document ID. Ignored if `collection_name` is unset. */
            doc_id?: string;
          }[];
          /** @description Vector store name. */
          collection_name?: string;
          /**
           * @description Selected embedding model.
           * @default clip
           * @enum {string}
           */
          model?: "clip";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated embeddings. */
            embeddings: {
              /** @description Embedding vector. */
              vector: number[];
              /** @description Vector store document ID. */
              doc_id?: string;
              /** @description Vector store document metadata. */
              metadata?: {
                [key: string]: unknown;
              };
            }[];
          };
        };
      };
    };
  };
  /**
   * JinaV2
   * @description Generate embeddings for multiple text documents using [Jina Embeddings 2](https://arxiv.org/abs/2310.19923).
   */
  JinaV2: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "items": [
         *     {
         *       "text": "Hassium is a superheavy element; it has been produced in a laboratory only in very small quantities by fusing heavy nuclei with lighter ones. Natural occurrences of the element have been hypothesised but never found.",
         *       "metadata": {
         *         "group": "8"
         *       }
         *     },
         *     {
         *       "text": "Xenon is also used to search for hypothetical weakly interacting massive particles and as a propellant for ion thrusters in spacecraft.",
         *       "metadata": {
         *         "group": "18"
         *       }
         *     }
         *   ],
         *   "collection_name": "smoke_tests",
         *   "embedded_metadata_keys": [
         *     "group"
         *   ]
         * }
         */
        "application/json": {
          /** @description Items to embed. */
          items: {
            /** @description Text to embed. */
            text: string;
            /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
            metadata?: {
              [key: string]: unknown;
            };
            /** @description Vector store document ID. Ignored if `collection_name` is unset. */
            doc_id?: string;
          }[];
          /** @description Vector store name. */
          collection_name?: string;
          /** @description Choose keys from `metadata` to embed with text. */
          embedded_metadata_keys?: string[];
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated embeddings. */
            embeddings: {
              /** @description Embedding vector. */
              vector: number[];
              /** @description Vector store document ID. */
              doc_id?: string;
              /** @description Vector store document metadata. */
              metadata?: {
                [key: string]: unknown;
              };
            }[];
          };
        };
      };
    };
  };
  /**
   * CLIP
   * @description Generate embeddings for text or images using [CLIP](https://openai.com/research/clip).
   */
  CLIP: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "items": [
         *     {
         *       "image_uri": "https://media.substrate.run/docs-fuji-red.jpg"
         *     },
         *     {
         *       "image_uri": "https://media.substrate.run/docs-fuji-blue.jpg"
         *     }
         *   ],
         *   "collection_name": "smoke_tests"
         * }
         */
        "application/json": {
          /** @description Items to embed. */
          items: {
            /** @description Image to embed. */
            image_uri?: string;
            /** @description Text to embed. */
            text?: string;
            /** @description Metadata that can be used to query the vector store. Ignored if `collection_name` is unset. */
            metadata?: {
              [key: string]: unknown;
            };
            /** @description Vector store document ID. Ignored if `collection_name` is unset. */
            doc_id?: string;
          }[];
          /** @description Vector store name. */
          collection_name?: string;
          /** @description Choose keys from `metadata` to embed with text. Only applies to text items. */
          embedded_metadata_keys?: string[];
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated embeddings. */
            embeddings: {
              /** @description Embedding vector. */
              vector: number[];
              /** @description Vector store document ID. */
              doc_id?: string;
              /** @description Vector store document metadata. */
              metadata?: {
                [key: string]: unknown;
              };
            }[];
          };
        };
      };
    };
  };
  /**
   * FindOrCreateVectorStore
   * @description Find a vector store matching the given collection name, or create a new vector store.
   */
  FindOrCreateVectorStore: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "collection_name": "smoke_tests",
         *   "model": "jina-v2"
         * }
         */
        "application/json": {
          /** @description Vector store name. */
          collection_name: string;
          /**
           * @description Selected embedding model.
           * @enum {string}
           */
          model: "jina-v2" | "clip";
        };
      };
    };
    responses: {
      /** @description Vector store created. */
      200: {
        content: {
          "application/json": {
            /** @description Vector store name. */
            collection_name: string;
            /**
             * @description Selected embedding model.
             * @enum {string}
             */
            model: "jina-v2" | "clip";
            /** @description Number of leaves in the vector store. */
            num_leaves?: number;
          };
        };
      };
    };
  };
  /**
   * ListVectorStores
   * @description List all vector stores.
   */
  ListVectorStores: {
    requestBody?: {
      content: {
        /** @example {} */
        "application/json": Record<string, never>;
      };
    };
    responses: {
      /** @description List of vector stores. */
      200: {
        content: {
          "application/json": {
            /** @description List of vector stores. */
            items?: {
              /** @description Vector store name. */
              collection_name: string;
              /**
               * @description Selected embedding model.
               * @enum {string}
               */
              model: "jina-v2" | "clip";
              /** @description Number of leaves in the vector store. */
              num_leaves?: number;
            }[];
          };
        };
      };
    };
  };
  /**
   * DeleteVectorStore
   * @description Delete a vector store.
   */
  DeleteVectorStore: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "collection_name": "fake_store",
         *   "model": "jina-v2"
         * }
         */
        "application/json": {
          /** @description Vector store name. */
          collection_name: string;
          /**
           * @description Selected embedding model.
           * @enum {string}
           */
          model: "jina-v2" | "clip";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Vector store name. */
            collection_name: string;
            /**
             * @description Selected embedding model.
             * @enum {string}
             */
            model: "jina-v2" | "clip";
          };
        };
      };
    };
  };
  /**
   * QueryVectorStore
   * @description Query a vector store for similar vectors.
   */
  QueryVectorStore: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "collection_name": "smoke_tests",
         *   "model": "jina-v2",
         *   "query_strings": [
         *     "gas",
         *     "metal"
         *   ],
         *   "top_k": 1,
         *   "include_metadata": true
         * }
         */
        "application/json": {
          /** @description Vector store to query against. */
          collection_name: string;
          /**
           * @description Selected embedding model.
           * @enum {string}
           */
          model: "jina-v2" | "clip";
          /** @description Texts to embed and use for the query. */
          query_strings?: string[];
          /** @description Image URIs to embed and use for the query. */
          query_image_uris?: string[];
          /** @description Vectors to use for the query. */
          query_vectors?: number[][];
          /** @description Document IDs to use for the query. */
          query_ids?: string[];
          /**
           * @description Number of results to return.
           * @default 10
           */
          top_k?: number;
          /**
           * @description The size of the dynamic candidate list for searching the index graph.
           * @default 40
           */
          ef_search?: number;
          /**
           * @description The number of leaves in the index tree to search.
           * @default 40
           */
          num_leaves_to_search?: number;
          /**
           * @description Include the values of the vectors in the response.
           * @default false
           */
          include_values?: boolean;
          /**
           * @description Include the metadata of the vectors in the response.
           * @default false
           */
          include_metadata?: boolean;
          /** @description Filter metadata by key-value pairs. */
          filters?: {
            [key: string]: unknown;
          };
        };
      };
    };
    responses: {
      /** @description Query results. */
      200: {
        content: {
          "application/json": {
            /** @description Query results. */
            results: {
              /** @description Document ID. */
              id: string;
              /**
               * Format: float
               * @description Similarity score.
               */
              distance: number;
              /** @description Embedding vector. */
              vector?: number[];
              /** @description Document metadata. */
              metadata?: {
                [key: string]: unknown;
              };
            }[][];
            /** @description Vector store name. */
            collection_name?: string;
            /**
             * @description Selected embedding model.
             * @enum {string}
             */
            model?: "jina-v2" | "clip";
          };
        };
      };
    };
  };
  /**
   * FetchVectors
   * @description Fetch vectors from a vector store.
   */
  FetchVectors: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "collection_name": "smoke_tests",
         *   "model": "jina-v2",
         *   "ids": [
         *     "dd8f3774e05d42caa53cfbaa7389c08f"
         *   ]
         * }
         */
        "application/json": {
          /** @description Vector store name. */
          collection_name: string;
          /**
           * @description Selected embedding model.
           * @enum {string}
           */
          model: "jina-v2" | "clip";
          /** @description Document IDs to retrieve. */
          ids: string[];
        };
      };
    };
    responses: {
      /** @description Vector data. */
      200: {
        content: {
          "application/json": {
            /** @description Retrieved vectors. */
            vectors: {
              /** @description Document ID. */
              id: string;
              /** @description Embedding vector. */
              vector: number[];
              /** @description Document metadata. */
              metadata: {
                [key: string]: unknown;
              };
            }[];
          };
        };
      };
    };
  };
  /**
   * UpdateVectors
   * @description Update vectors in a vector store.
   */
  UpdateVectors: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "collection_name": "smoke_tests",
         *   "model": "jina-v2",
         *   "vectors": [
         *     {
         *       "id": "dd8f3774e05d42caa53cfbaa7389c08f",
         *       "metadata": {
         *         "appearance": "silvery, blue cast"
         *       }
         *     }
         *   ]
         * }
         */
        "application/json": {
          /** @description Vector store name. */
          collection_name: string;
          /**
           * @description Selected embedding model.
           * @enum {string}
           */
          model: "jina-v2" | "clip";
          /** @description Vectors to upsert. */
          vectors: {
            /** @description Document ID. */
            id: string;
            /** @description Embedding vector. */
            vector?: number[];
            /** @description Document metadata. */
            metadata?: {
              [key: string]: unknown;
            };
          }[];
        };
      };
    };
    responses: {
      /** @description Count of updated vectors. */
      200: {
        content: {
          "application/json": {
            /** @description Number of vectors modified. */
            count: number;
          };
        };
      };
    };
  };
  /**
   * DeleteVectors
   * @description Delete vectors in a vector store.
   */
  DeleteVectors: {
    requestBody?: {
      content: {
        /**
         * @example {
         *   "collection_name": "smoke_tests",
         *   "model": "jina-v2",
         *   "ids": [
         *     "ac32b9a133dd4e3689004f6e8f0fd6cd",
         *     "629df177c7644062a68bceeff223cefa"
         *   ]
         * }
         */
        "application/json": {
          /** @description Vector store name. */
          collection_name: string;
          /**
           * @description Selected embedding model.
           * @enum {string}
           */
          model: "jina-v2" | "clip";
          /** @description Document IDs to delete. */
          ids: string[];
        };
      };
    };
    responses: {
      /** @description Count of deleted vectors. */
      200: {
        content: {
          "application/json": {
            /** @description Number of vectors modified. */
            count: number;
          };
        };
      };
    };
  };
}
