/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */


export interface paths {
  "/GenerateText": {
    /**
     * GenerateText
     * @description Generate text using a language model.
     */
    post: operations["GenerateText"];
  };
  "/MultiGenerateText": {
    /**
     * MultiGenerateText
     * @description Generate multiple text choices using a language model.
     */
    post: operations["MultiGenerateText"];
  };
  "/GenerateTextVision": {
    /**
     * GenerateTextVision
     * @description Generate text by prompting with text and images using a vision-language model.
     */
    post: operations["GenerateTextVision"];
  };
  "/GenerateImage": {
    /**
     * GenerateImage
     * @description Generate an image.
     */
    post: operations["GenerateImage"];
  };
  "/MultiGenerateImage": {
    /**
     * MultiGenerateImage
     * @description Generate multiple images.
     */
    post: operations["MultiGenerateImage"];
  };
  "/ControlledGenerateImage": {
    /**
     * ControlledGenerateImage
     * @description Generate an image with generation controlled by an input image.
     */
    post: operations["ControlledGenerateImage"];
  };
  "/MultiControlledGenerateImage": {
    /**
     * MultiControlledGenerateImage
     * @description Generate multiple image outputs with generation controlled by an input image.
     */
    post: operations["MultiControlledGenerateImage"];
  };
  "/GenerativeEditImage": {
    /**
     * GenerativeEditImage
     * @description Edit an image with a generative model.
     */
    post: operations["GenerativeEditImage"];
  };
  "/MultiGenerativeEditImage": {
    /**
     * MultiGenerativeEditImage
     * @description Generate multiple image outputs modifying part of an image using a mask.
     */
    post: operations["MultiGenerativeEditImage"];
  };
  "/FillMask": {
    /**
     * FillMask
     * @description Edit an image with a generative model.
     */
    post: operations["FillMask"];
  };
  "/UpscaleImage": {
    /**
     * UpscaleImage
     * @description Upscale an image.
     */
    post: operations["UpscaleImage"];
  };
  "/RemoveBackground": {
    /**
     * RemoveBackground
     * @description Remove the background from an image, with the option to return the foreground as a mask.
     */
    post: operations["RemoveBackground"];
  };
  "/DetectSegments": {
    /**
     * DetectSegments
     * @description Detect segments in an image given point(s) or bounding box(es).
     */
    post: operations["DetectSegments"];
  };
  "/TranscribeMedia": {
    /**
     * TranscribeMedia
     * @description Transcribe speech in an audio or video file.
     */
    post: operations["TranscribeMedia"];
  };
  "/GenerateSpeech": {
    /**
     * GenerateSpeech
     * @description Generate speech from text.
     */
    post: operations["GenerateSpeech"];
  };
  "/EmbedText": {
    /**
     * EmbedText
     * @description Generate vector embedding for a text document.
     */
    post: operations["EmbedText"];
  };
  "/MultiEmbedText": {
    /**
     * MultiEmbedText
     * @description Generate vector embeddings for multiple text documents.
     */
    post: operations["MultiEmbedText"];
  };
  "/EmbedImage": {
    /**
     * EmbedImage
     * @description Generate vector embedding for an image, and optionally store the embedding.
     */
    post: operations["EmbedImage"];
  };
  "/MultiEmbedImage": {
    /**
     * MultiEmbedImage
     * @description Generate vector embeddings for multiple images, and optionally store the embeddings.
     */
    post: operations["MultiEmbedImage"];
  };
  "/vector-stores/create": {
    /**
     * /vector-stores/create
     * @description Create a vector store for storing and querying embeddings.
     */
    post: operations["/vector-stores/create"];
  };
  "/vector-stores/list": {
    /**
     * /vector-stores/list
     * @description List all vector stores.
     */
    get: operations["/vector-stores/list"];
  };
  "/vector-stores/delete": {
    /**
     * /vector-stores/delete
     * @description Delete a vector store.
     */
    post: operations["/vector-stores/delete"];
  };
  "/vector-stores/query": {
    /**
     * /vector-stores/query
     * @description Query a vector store for similar vectors.
     */
    post: operations["/vector-stores/query"];
  };
  "/vectors/fetch": {
    /**
     * /vectors/fetch
     * @description Fetch vectors from a vector store.
     */
    post: operations["/vectors/fetch"];
  };
  "/vectors/update": {
    /**
     * /vectors/update
     * @description Update vectors in a vector store.
     */
    post: operations["/vectors/update"];
  };
  "/vectors/delete": {
    /**
     * /vectors/delete
     * @description Delete vectors in a vector store.
     */
    post: operations["/vectors/delete"];
  };
}

export type webhooks = Record<string, never>;

export interface components {
  schemas: {
    /** ErrorOut */
    ErrorOut: {
      /**
       * @description The type of error returned.
       * @enum {string}
       */
      type: "api_error" | "invalid_request_error";
      /** @description A message providing more details about the error. */
      message: string;
    };
    /** ResponseFormat */
    ResponseFormat: {
      /**
       * @description Type of response.
       * @default text
       * @enum {string}
       */
      type: "json_object" | "text";
      /** @description JSON schema to guide `json_object` response. */
      json_schema?: Record<string, never>;
    };
    /** GenerateTextIn */
    GenerateTextIn: {
      /** @description Input prompt. */
      prompt: string;
      /**
       * @description Selected model.
       * @default mistral-7b-instruct
       * @enum {string}
       */
      model?: "mistral-7b-instruct";
      /** ResponseFormat */
      response_format?: {
        /**
         * @description Type of response.
         * @default text
         * @enum {string}
         */
        type: "json_object" | "text";
        /** @description JSON schema to guide `json_object` response. */
        json_schema?: Record<string, never>;
      };
      /**
       * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
       * @default 4
       */
      temperature?: number;
      /**
       * @description Maximum number of tokens to generate.
       * @default 800
       */
      max_tokens?: number;
    };
    /** MultiGenerateTextIn */
    MultiGenerateTextIn: {
      /** @description Input prompt. */
      prompt: string;
      /** @description Number of choices to generate. */
      num_choices: number;
      /**
       * @description Selected model.
       * @default mistral-7b-instruct
       * @enum {string}
       */
      model?: "mistral-7b-instruct";
      /** ResponseFormat */
      response_format?: {
        /**
         * @description Type of response.
         * @default text
         * @enum {string}
         */
        type: "json_object" | "text";
        /** @description JSON schema to guide `json_object` response. */
        json_schema?: Record<string, never>;
      };
      /**
       * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
       * @default 4
       */
      temperature?: number;
      /**
       * @description Maximum number of tokens to generate.
       * @default 800
       */
      max_tokens?: number;
    };
    /** GenerateTextOut */
    GenerateTextOut: {
      /** @description Text response. */
      text?: string;
      /** @description JSON response. */
      json_object?: Record<string, never>;
    };
    /** MultiGenerateTextOut */
    MultiGenerateTextOut: {
      choices: {
          /** @description Text response. */
          text?: string;
          /** @description JSON response. */
          json_object?: Record<string, never>;
        }[];
    };
    /** GenerateTextVisionIn */
    GenerateTextVisionIn: {
      /** @description Text prompt. */
      prompt: string;
      /** @description Image prompts. */
      image_uris?: string[];
      /**
       * @description Selected model.
       * @default firellava-13b
       * @enum {string}
       */
      model?: "firellava-13b";
      /**
       * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
       * @default 4
       */
      temperature?: number;
      /**
       * @description Maximum number of tokens to generate.
       * @default 800
       */
      max_tokens?: number;
    };
    /** GenerateTextVisionOut */
    GenerateTextVisionOut: {
      /** @description Text response. */
      text: string;
    };
    /** GenerateImageIn */
    GenerateImageIn: {
      /** @description Text prompt. */
      prompt: string;
      /** @description Image prompt. */
      image_prompt_uri?: string;
      /**
       * @description Selected model.
       * @default stablediffusion-xl
       * @enum {string}
       */
      model?: "stablediffusion-xl";
      /**
       * Format: float
       * @description Controls the influence of the image prompt on the generated output.
       * @default 5
       */
      image_influence?: number;
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /** @description Width of output image, in pixels. */
      width?: number;
      /** @description Height of output image, in pixels. */
      height?: number;
      /** @description Seed for deterministic generation. Default is a random seed. */
      seed?: number;
    };
    /** GenerateImageOut */
    GenerateImageOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
      /** @description The random noise seed used for generation. */
      seed: number;
    };
    /** MultiGenerateImageIn */
    MultiGenerateImageIn: {
      /** @description Text prompt. */
      prompt: string;
      /** @description Image prompt. */
      image_prompt_uri?: string;
      /** @description Number of images to generate. */
      num_images: number;
      /**
       * @description Selected model.
       * @default stablediffusion-xl
       * @enum {string}
       */
      model?: "stablediffusion-xl";
      /**
       * Format: float
       * @description Controls the influence of the image prompt on the generated output.
       * @default 5
       */
      image_influence?: number;
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /** @description Width of output image, in pixels. */
      width?: number;
      /** @description Height of output image, in pixels. */
      height?: number;
      /** @description Random noise seeds. Default is random seeds for each generation. */
      seeds?: number[];
    };
    /** MultiGenerateImageOut */
    MultiGenerateImageOut: {
      outputs: {
          /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
          image_uri: string;
          /** @description The random noise seed used for generation. */
          seed: number;
        }[];
    };
    /** ControlledGenerateImageIn */
    ControlledGenerateImageIn: {
      /** @description Input image. */
      image_uri: string;
      /**
       * @description Strategy to control generation using the input image.
       * @enum {string}
       */
      control_method: "edge" | "depth" | "illusion";
      /** @description Text prompt. */
      prompt: string;
      /**
       * @description Resolution of the output image, in pixels.
       * @default 1024
       */
      output_resolution?: number;
      /**
       * @description Selected model.
       * @default stablediffusion-xl
       * @enum {string}
       */
      model?: "stablediffusion-xl";
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /**
       * Format: float
       * @description Controls the influence of the input image on the generated output.
       * @default 9
       */
      image_influence?: number;
      /** @description Seed for deterministic generation. Default is a random seed. */
      seed?: number;
    };
    /** ControlledGenerateImageOut */
    ControlledGenerateImageOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
      /** @description The random noise seed used for generation. */
      seed: number;
    };
    /** MultiControlledGenerateImageIn */
    MultiControlledGenerateImageIn: {
      /** @description Input image. */
      image_uri: string;
      /**
       * @description Strategy to control generation using the input image.
       * @enum {string}
       */
      control_method: "edge" | "depth" | "illusion";
      /** @description Text prompt. */
      prompt: string;
      /** @description Number of images to generate. */
      num_images: number;
      /**
       * @description Resolution of the output image, in pixels.
       * @default 1024
       */
      output_resolution?: number;
      /**
       * @description Selected model.
       * @default stablediffusion-xl
       * @enum {string}
       */
      model?: "stablediffusion-xl";
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /**
       * Format: float
       * @description Controls the influence of the input image on the generated output.
       * @default 9
       */
      image_influence?: number;
      /** @description Random noise seeds. Default is random seeds for each generation. */
      seeds?: number[];
    };
    /** MultiControlledGenerateImageOut */
    MultiControlledGenerateImageOut: {
      outputs: {
          /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
          image_uri: string;
          /** @description The random noise seed used for generation. */
          seed: number;
        }[];
    };
    /** GenerativeEditImageIn */
    GenerativeEditImageIn: {
      /** @description Original image. */
      image_uri: string;
      /** @description Text prompt. */
      prompt: string;
      /** @description Mask image that controls which pixels are inpainted. If unset, the entire image is edited (image-to-image). */
      mask_image_uri?: string;
      /** @description Image prompt. */
      image_prompt_uri?: string;
      /**
       * @description Resolution of the output image, in pixels.
       * @default 1024
       */
      output_resolution?: number;
      /**
       * @description Selected model.
       * @default stablediffusion-xl
       * @enum {string}
       */
      model?: "stablediffusion-xl";
      /**
       * Format: float
       * @description Controls the strength of the generation process.
       * @default 8
       */
      strength?: number;
      /**
       * Format: float
       * @description Controls the influence of the image prompt on the generated output.
       * @default 5
       */
      image_prompt_influence?: number;
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /** @description Seed for deterministic generation. Default is a random seed. */
      seed?: number;
    };
    /** GenerativeEditImageOut */
    GenerativeEditImageOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
      /** @description The random noise seed used for generation. */
      seed: number;
    };
    /** MultiGenerativeEditImageIn */
    MultiGenerativeEditImageIn: {
      /** @description Original image. */
      image_uri: string;
      /** @description Text prompt. */
      prompt: string;
      /** @description Mask image that controls which pixels are edited (inpainting). If unset, the entire image is edited (image-to-image). */
      mask_image_uri?: string;
      /** @description Image prompt. */
      image_prompt_uri?: string;
      /** @description Number of images to generate. */
      num_images: number;
      /**
       * @description Resolution of the output image, in pixels.
       * @default 1024
       */
      output_resolution?: number;
      /**
       * @description Selected model.
       * @default stablediffusion-xl
       * @enum {string}
       */
      model?: "stablediffusion-xl";
      /** @description Negative input prompt. */
      negative_prompt?: string;
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
      /**
       * Format: float
       * @description Controls the strength of the generation process.
       * @default 8
       */
      strength?: number;
      /**
       * Format: float
       * @description Controls the influence of the image prompt on the generated output.
       * @default 5
       */
      image_prompt_influence?: number;
      /** @description Random noise seeds. Default is random seeds for each generation. */
      seeds?: number[];
    };
    /** MultiGenerativeEditImageOut */
    MultiGenerativeEditImageOut: {
      outputs: {
          /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
          image_uri: string;
          /** @description The random noise seed used for generation. */
          seed: number;
        }[];
    };
    /** BoundingBox */
    BoundingBox: {
      /**
       * Format: float
       * @description Top left corner x.
       */
      x1: number;
      /**
       * Format: float
       * @description Top left corner y.
       */
      y1: number;
      /**
       * Format: float
       * @description Bottom right corner x.
       */
      x2: number;
      /**
       * Format: float
       * @description Bottom right corner y.
       */
      y2: number;
    };
    /** Point */
    Point: {
      /** @description X position. */
      x: number;
      /** @description Y position. */
      y: number;
    };
    /** FillMaskIn */
    FillMaskIn: {
      /** @description Input image. */
      image_uri: string;
      /** @description Mask image that controls which pixels are inpainted. */
      mask_image_uri: string;
      /**
       * @description Selected model.
       * @default big-lama
       * @enum {string}
       */
      model?: "big-lama";
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** FillMaskOut */
    FillMaskOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
    };
    /** RemoveBackgroundIn */
    RemoveBackgroundIn: {
      /** @description Input image. */
      image_uri: string;
      /** @description Return a mask image instead of the original content. */
      return_mask?: boolean;
      /** @description Hex value background color. Transparent if unset. */
      background_color?: string;
      /**
       * @description Selected model.
       * @default isnet
       * @enum {string}
       */
      model?: "isnet";
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** RemoveBackgroundOut */
    RemoveBackgroundOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
    };
    /** UpscaleImageIn */
    UpscaleImageIn: {
      /** @description Input image. */
      image_uri: string;
      /**
       * @description Selected model.
       * @default real-esrgan-x4
       * @enum {string}
       */
      model?: "real-esrgan-x4";
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** UpscaleImageOut */
    UpscaleImageOut: {
      /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      image_uri: string;
    };
    /** DetectSegmentsIn */
    DetectSegmentsIn: {
      /** @description Input image. */
      image_uri: string;
      /** @description Point prompts, to detect a segment under the point. One of `point_prompt` or `box_prompt` must be set. */
      point_prompts?: {
          /** @description X position. */
          x: number;
          /** @description Y position. */
          y: number;
        }[];
      /** @description Box prompts, to detect a segment within the bounding box. One of `point_prompt` or `box_prompt` must be set. */
      box_prompts?: {
          /**
           * Format: float
           * @description Top left corner x.
           */
          x1: number;
          /**
           * Format: float
           * @description Top left corner y.
           */
          y1: number;
          /**
           * Format: float
           * @description Bottom right corner x.
           */
          x2: number;
          /**
           * Format: float
           * @description Bottom right corner y.
           */
          y2: number;
        }[];
      /**
       * @description Selected model.
       * @default segment-anything
       * @enum {string}
       */
      model?: "segment-anything";
      /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** DetectSegmentsOut */
    DetectSegmentsOut: {
      /** @description Detected segments in 'mask image' format. Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
      mask_image_uri: string;
    };
    /** TranscribeMediaIn */
    TranscribeMediaIn: {
      /** @description Input audio. */
      audio_uri: string;
      /** @description Prompt to guide model on the content and context of input audio. */
      prompt?: string;
      /**
       * @description Language of input audio in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format.
       * @default en
       */
      language?: string;
      /**
       * @description Segment the text into sentences with approximate timestamps.
       * @default false
       */
      segment?: boolean;
      /**
       * @description Align transcription to produce more accurate sentence-level timestamps and word-level timestamps. An array of word segments will be included in each sentence segment.
       * @default false
       */
      align?: boolean;
      /**
       * @description Identify speakers for each segment. Speaker IDs will be included in each segment.
       * @default false
       */
      diarize?: boolean;
      /**
       * @description Suggest automatic chapter markers.
       * @default false
       */
      suggest_chapters?: boolean;
    };
    /** TranscribedWord */
    TranscribedWord: {
      /** @description Text of word. */
      word: string;
      /**
       * Format: float
       * @description Start time of word, in seconds.
       */
      start?: number;
      /**
       * Format: float
       * @description End time of word, in seconds.
       */
      end?: number;
      /** @description ID of speaker, if `diarize` is enabled. */
      speaker?: string;
    };
    /** TranscribedSegment */
    TranscribedSegment: {
      /** @description Text of segment. */
      text: string;
      /**
       * Format: float
       * @description Start time of segment, in seconds.
       */
      start: number;
      /**
       * Format: float
       * @description End time of segment, in seconds.
       */
      end: number;
      /** @description ID of speaker, if `diarize` is enabled. */
      speaker?: string;
      /** @description Aligned words, if `align` is enabled. */
      words?: {
          /** @description Text of word. */
          word: string;
          /**
           * Format: float
           * @description Start time of word, in seconds.
           */
          start?: number;
          /**
           * Format: float
           * @description End time of word, in seconds.
           */
          end?: number;
          /** @description ID of speaker, if `diarize` is enabled. */
          speaker?: string;
        }[];
    };
    /** ChapterMarker */
    ChapterMarker: {
      /** @description Chapter title. */
      title: string;
      /**
       * Format: float
       * @description Start time of chapter, in seconds.
       */
      start: number;
    };
    /** TranscribeMediaOut */
    TranscribeMediaOut: {
      /** @description Transcribed text. */
      text: string;
      /** @description Transcribed segments, if `segment` is enabled. */
      segments?: {
          /** @description Text of segment. */
          text: string;
          /**
           * Format: float
           * @description Start time of segment, in seconds.
           */
          start: number;
          /**
           * Format: float
           * @description End time of segment, in seconds.
           */
          end: number;
          /** @description ID of speaker, if `diarize` is enabled. */
          speaker?: string;
          /** @description Aligned words, if `align` is enabled. */
          words?: {
              /** @description Text of word. */
              word: string;
              /**
               * Format: float
               * @description Start time of word, in seconds.
               */
              start?: number;
              /**
               * Format: float
               * @description End time of word, in seconds.
               */
              end?: number;
              /** @description ID of speaker, if `diarize` is enabled. */
              speaker?: string;
            }[];
        }[];
      /** @description Chapter markers, if `suggest_chapters` is enabled. */
      chapters?: {
          /** @description Chapter title. */
          title: string;
          /**
           * Format: float
           * @description Start time of chapter, in seconds.
           */
          start: number;
        }[];
    };
    /** GenerateSpeechIn */
    GenerateSpeechIn: {
      /** @description Input text. */
      text: string;
      /** @description Reference audio used to synthesize the speaker. If unset, a default speaker voice will be used. */
      audio_uri?: string;
      /**
       * @description Language of input text. Supported languages: `en, de, fr, es, it, pt, pl, zh, ar, cs, ru, nl, tr, hu, ko`.
       * @default en
       */
      language?: string;
      /** @description Use "hosted" to return an audio URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the audio data will be returned as a base64-encoded string. */
      store?: string;
    };
    /** GenerateSpeechOut */
    GenerateSpeechOut: {
      /** @description Base 64-encoded WAV audio bytes, or a hosted audio url if `store` is provided. */
      audio_uri: string;
    };
    /** Embedding */
    Embedding: {
      /** @description Embedding vector. */
      vector: string;
    };
    /** EmbedTextIn */
    EmbedTextIn: {
      /** @description Text to embed. */
      text: string;
      /**
       * @description Selected model.
       * @default jina-v2
       * @enum {string}
       */
      model?: "jina-v2" | "clip";
      /** @description [Vector store](/docs/vector-stores) identifier. */
      store?: string;
      /** @description Metadata that can be used to query the vector store. Ignored if `store` is unset. */
      metadata?: Record<string, never>;
      embedded_metadata?: {
        /** @description Keys to embed with text. */
        include_keys?: string[];
        /** @description Keys to exclude. All other keys will be embedded with text. */
        exclude_keys?: string[];
      };
    };
    /** EmbedTextOut */
    EmbedTextOut: {
      /** Embedding */
      embedding: {
        /** @description Embedding vector. */
        vector: string;
      };
    };
    EmbedTextItem: {
      /** @description Text to embed. */
      text: string;
      /** @description Metadata that can be used to query the vector store. Ignored if `store` is unset. */
      metadata?: Record<string, never>;
    };
    /** MultiEmbedTextIn */
    MultiEmbedTextIn: {
      /** @description Items to embed. */
      items: {
          /** @description Text to embed. */
          text: string;
          /** @description Metadata that can be used to query the vector store. Ignored if `store` is unset. */
          metadata?: Record<string, never>;
        }[];
      /**
       * @description Selected model.
       * @default jina-v2
       * @enum {string}
       */
      model?: "jina-v2" | "clip";
      /** @description [Vector store](/docs/vector-stores) identifier. */
      store?: string;
      embedded_metadata?: {
        /** @description Keys to embed with text. */
        include_keys?: string[];
        /** @description Keys to exclude. All other keys will be embedded with text. */
        exclude_keys?: string[];
      };
    };
    /** MultiEmbedTextOut */
    MultiEmbedTextOut: {
      /** @description Generated embeddings. */
      embeddings: {
          /** @description Embedding vector. */
          vector: string;
        }[];
    };
    EmbeddedMetadataSelect: {
      /** @description Keys to embed with text. */
      include_keys?: string[];
      /** @description Keys to exclude. All other keys will be embedded with text. */
      exclude_keys?: string[];
    };
    /** EmbedImageIn */
    EmbedImageIn: {
      /** @description Image to embed. */
      image_uri: string;
      /**
       * @description Selected model.
       * @default clip
       * @enum {string}
       */
      model?: "clip";
      /** @description [Vector store](/docs/vector-stores) identifier. */
      store?: string;
    };
    /** EmbedImageOut */
    EmbedImageOut: {
      /** Embedding */
      embedding: {
        /** @description Embedding vector. */
        vector: string;
      };
    };
    EmbedImageItem: {
      /** @description Image to embed. */
      image_uri: string;
    };
    /** MultiEmbedImageIn */
    MultiEmbedImageIn: {
      /** @description Items to embed. */
      items: {
          /** @description Image to embed. */
          image_uri: string;
        }[];
      /** @description [Vector store](/docs/vector-stores) identifier. */
      store?: string;
      /**
       * @description Selected model.
       * @default clip
       * @enum {string}
       */
      model?: "clip";
    };
    /** MultiEmbedImageOut */
    MultiEmbedImageOut: {
      /** @description Generated embeddings. */
      embeddings: {
          /** @description Embedding vector. */
          vector: string;
        }[];
    };
    /**
     * VectorStoreParams
     * @description Fields describing a vector store and its associated index.
     */
    VectorStoreParams: {
      /** @description Vector store name. */
      name: string;
      /**
       * @description Selected embedding model
       * @enum {string}
       */
      model: "jina-v2" | "clip";
      /**
       * @description The max number of connections per layer for the index.
       * @default 16
       */
      m?: number;
      /**
       * @description The size of the dynamic candidate list for constructing the index graph.
       * @default 64
       */
      ef_construction?: number;
      /**
       * @description The distance metric to construct the index with.
       * @default inner
       * @enum {string}
       */
      metric?: "cosine" | "l2" | "inner";
    };
    /** DeleteVectorStoreParams */
    DeleteVectorStoreParams: {
      /** @description Vector store name. */
      name: string;
      /**
       * @description Selected embedding model
       * @enum {string}
       */
      model: "jina-v2" | "clip";
    };
    /**
     * Vector
     * @description Canonical representation of document with embedding vector.
     */
    Vector: {
      /** @description Document ID. */
      id: string;
      /** @description Embedding vector. */
      vector: number[];
      /** @description Document metadata. */
      metadata: Record<string, never>;
    };
    /** GetVectorsParams */
    GetVectorsParams: {
      /** @description Vector store name. */
      name: string;
      /**
       * @description Selected embedding model
       * @enum {string}
       */
      model: "jina-v2" | "clip";
      /** @description Document IDs to retrieve. */
      ids: string[];
    };
    /** GetVectorsResponse */
    GetVectorsResponse: {
      /** @description Retrieved vectors. */
      vectors: {
          /** @description Document ID. */
          id: string;
          /** @description Embedding vector. */
          vector: number[];
          /** @description Document metadata. */
          metadata: Record<string, never>;
        }[];
    };
    /** VectorUpdateCountResponse */
    VectorUpdateCountResponse: {
      /** @description Number of vectors modified. */
      count: number;
    };
    /**
     * UpdateVectorParams
     * @description Document to update.
     */
    UpdateVectorParams: {
      /** @description Document ID. */
      id: string;
      /** @description Embedding vector. */
      vector?: number[];
      /** @description Document metadata. */
      metadata?: Record<string, never>;
    };
    /** UpdateVectorsParams */
    UpdateVectorsParams: {
      /** @description Vector store name. */
      name: string;
      /**
       * @description Selected embedding model
       * @enum {string}
       */
      model: "jina-v2" | "clip";
      /** @description Vectors to upsert. */
      vectors: {
          /** @description Document ID. */
          id: string;
          /** @description Embedding vector. */
          vector?: number[];
          /** @description Document metadata. */
          metadata?: Record<string, never>;
        }[];
    };
    /** DeleteVectorsParams */
    DeleteVectorsParams: {
      /** @description Vector store name. */
      name: string;
      /**
       * @description Selected embedding model
       * @enum {string}
       */
      model: "jina-v2" | "clip";
      /** @description Document IDs to delete. */
      ids: string[];
    };
    /** QueryVectorStoreParams */
    QueryVectorStoreParams: {
      /** @description Vector store to query against. */
      name: string;
      /**
       * @description Selected embedding model
       * @enum {string}
       */
      model: "jina-v2" | "clip";
      /** @description Document IDs to use for the query. */
      query_ids?: string[];
      /** @description Image URIs to embed and use for the query. */
      query_image_uris?: string[];
      /** @description Vector to use for the query. */
      query_vectors?: number[][];
      /** @description Texts to embed and use for the query. */
      query_strings?: string[];
      /**
       * @description Number of results to return.
       * @default 10
       */
      top_k?: number;
      /**
       * @description The size of the dynamic candidate list for searching the index graph.
       * @default 40
       */
      ef_search?: number;
      /**
       * @description Include the values of the vectors in the response.
       * @default false
       */
      include_values?: boolean;
      /**
       * @description Include the metadata of the vectors in the response.
       * @default false
       */
      include_metadata?: boolean;
      /** @description Filter metadata by key-value pairs. */
      filters?: Record<string, never>;
    };
    /** VectorStoreQueryResult */
    VectorStoreQueryResult: {
      /** @description Document ID. */
      id: string;
      /**
       * Format: float
       * @description Similarity score.
       */
      distance: number;
      /** @description Embedding vector. */
      vector?: number[];
      /** @description Document metadata. */
      metadata?: Record<string, never>;
    };
    /** QueryVectorStoreResponse */
    QueryVectorStoreResponse: {
      /** @description Query results. */
      results: {
            /** @description Document ID. */
            id: string;
            /**
             * Format: float
             * @description Similarity score.
             */
            distance: number;
            /** @description Embedding vector. */
            vector?: number[];
            /** @description Document metadata. */
            metadata?: Record<string, never>;
          }[][];
      /** @description Vector store name. */
      name?: string;
      /**
       * @description Selected embedding model
       * @enum {string}
       */
      model?: "jina-v2" | "clip";
      /**
       * @description The distance metric used for the query.
       * @enum {string}
       */
      metric?: "cosine" | "l2" | "inner";
    };
  };
  responses: never;
  parameters: never;
  requestBodies: never;
  headers: never;
  pathItems: never;
}

export type $defs = Record<string, never>;

export type external = Record<string, never>;

export interface operations {

  /**
   * GenerateText
   * @description Generate text using a language model.
   */
  GenerateText: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Input prompt. */
          prompt: string;
          /**
           * @description Selected model.
           * @default mistral-7b-instruct
           * @enum {string}
           */
          model?: "mistral-7b-instruct";
          /** ResponseFormat */
          response_format?: {
            /**
             * @description Type of response.
             * @default text
             * @enum {string}
             */
            type: "json_object" | "text";
            /** @description JSON schema to guide `json_object` response. */
            json_schema?: Record<string, never>;
          };
          /**
           * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
           * @default 4
           */
          temperature?: number;
          /**
           * @description Maximum number of tokens to generate.
           * @default 800
           */
          max_tokens?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Text response. */
            text?: string;
            /** @description JSON response. */
            json_object?: Record<string, never>;
          };
        };
      };
    };
  };
  /**
   * MultiGenerateText
   * @description Generate multiple text choices using a language model.
   */
  MultiGenerateText: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Input prompt. */
          prompt: string;
          /** @description Number of choices to generate. */
          num_choices: number;
          /**
           * @description Selected model.
           * @default mistral-7b-instruct
           * @enum {string}
           */
          model?: "mistral-7b-instruct";
          /** ResponseFormat */
          response_format?: {
            /**
             * @description Type of response.
             * @default text
             * @enum {string}
             */
            type: "json_object" | "text";
            /** @description JSON schema to guide `json_object` response. */
            json_schema?: Record<string, never>;
          };
          /**
           * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
           * @default 4
           */
          temperature?: number;
          /**
           * @description Maximum number of tokens to generate.
           * @default 800
           */
          max_tokens?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            choices: {
                /** @description Text response. */
                text?: string;
                /** @description JSON response. */
                json_object?: Record<string, never>;
              }[];
          };
        };
      };
    };
  };
  /**
   * GenerateTextVision
   * @description Generate text by prompting with text and images using a vision-language model.
   */
  GenerateTextVision: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Text prompt. */
          prompt: string;
          /** @description Image prompts. */
          image_uris?: string[];
          /**
           * @description Selected model.
           * @default firellava-13b
           * @enum {string}
           */
          model?: "firellava-13b";
          /**
           * @description Sampling temperature to use. Higher values make the output more random, lower values make the output more deterministic.
           * @default 4
           */
          temperature?: number;
          /**
           * @description Maximum number of tokens to generate.
           * @default 800
           */
          max_tokens?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Text response. */
            text: string;
          };
        };
      };
    };
  };
  /**
   * GenerateImage
   * @description Generate an image.
   */
  GenerateImage: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Text prompt. */
          prompt: string;
          /** @description Image prompt. */
          image_prompt_uri?: string;
          /**
           * @description Selected model.
           * @default stablediffusion-xl
           * @enum {string}
           */
          model?: "stablediffusion-xl";
          /**
           * Format: float
           * @description Controls the influence of the image prompt on the generated output.
           * @default 5
           */
          image_influence?: number;
          /** @description Negative input prompt. */
          negative_prompt?: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
          /** @description Width of output image, in pixels. */
          width?: number;
          /** @description Height of output image, in pixels. */
          height?: number;
          /** @description Seed for deterministic generation. Default is a random seed. */
          seed?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
            /** @description The random noise seed used for generation. */
            seed: number;
          };
        };
      };
    };
  };
  /**
   * MultiGenerateImage
   * @description Generate multiple images.
   */
  MultiGenerateImage: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Text prompt. */
          prompt: string;
          /** @description Image prompt. */
          image_prompt_uri?: string;
          /** @description Number of images to generate. */
          num_images: number;
          /**
           * @description Selected model.
           * @default stablediffusion-xl
           * @enum {string}
           */
          model?: "stablediffusion-xl";
          /**
           * Format: float
           * @description Controls the influence of the image prompt on the generated output.
           * @default 5
           */
          image_influence?: number;
          /** @description Negative input prompt. */
          negative_prompt?: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
          /** @description Width of output image, in pixels. */
          width?: number;
          /** @description Height of output image, in pixels. */
          height?: number;
          /** @description Random noise seeds. Default is random seeds for each generation. */
          seeds?: number[];
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            outputs: {
                /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
                image_uri: string;
                /** @description The random noise seed used for generation. */
                seed: number;
              }[];
          };
        };
      };
    };
  };
  /**
   * ControlledGenerateImage
   * @description Generate an image with generation controlled by an input image.
   */
  ControlledGenerateImage: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Input image. */
          image_uri: string;
          /**
           * @description Strategy to control generation using the input image.
           * @enum {string}
           */
          control_method: "edge" | "depth" | "illusion";
          /** @description Text prompt. */
          prompt: string;
          /**
           * @description Resolution of the output image, in pixels.
           * @default 1024
           */
          output_resolution?: number;
          /**
           * @description Selected model.
           * @default stablediffusion-xl
           * @enum {string}
           */
          model?: "stablediffusion-xl";
          /** @description Negative input prompt. */
          negative_prompt?: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
          /**
           * Format: float
           * @description Controls the influence of the input image on the generated output.
           * @default 9
           */
          image_influence?: number;
          /** @description Seed for deterministic generation. Default is a random seed. */
          seed?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
            /** @description The random noise seed used for generation. */
            seed: number;
          };
        };
      };
    };
  };
  /**
   * MultiControlledGenerateImage
   * @description Generate multiple image outputs with generation controlled by an input image.
   */
  MultiControlledGenerateImage: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Input image. */
          image_uri: string;
          /**
           * @description Strategy to control generation using the input image.
           * @enum {string}
           */
          control_method: "edge" | "depth" | "illusion";
          /** @description Text prompt. */
          prompt: string;
          /** @description Number of images to generate. */
          num_images: number;
          /**
           * @description Resolution of the output image, in pixels.
           * @default 1024
           */
          output_resolution?: number;
          /**
           * @description Selected model.
           * @default stablediffusion-xl
           * @enum {string}
           */
          model?: "stablediffusion-xl";
          /** @description Negative input prompt. */
          negative_prompt?: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
          /**
           * Format: float
           * @description Controls the influence of the input image on the generated output.
           * @default 9
           */
          image_influence?: number;
          /** @description Random noise seeds. Default is random seeds for each generation. */
          seeds?: number[];
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            outputs: {
                /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
                image_uri: string;
                /** @description The random noise seed used for generation. */
                seed: number;
              }[];
          };
        };
      };
    };
  };
  /**
   * GenerativeEditImage
   * @description Edit an image with a generative model.
   */
  GenerativeEditImage: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Original image. */
          image_uri: string;
          /** @description Text prompt. */
          prompt: string;
          /** @description Mask image that controls which pixels are inpainted. If unset, the entire image is edited (image-to-image). */
          mask_image_uri?: string;
          /** @description Image prompt. */
          image_prompt_uri?: string;
          /**
           * @description Resolution of the output image, in pixels.
           * @default 1024
           */
          output_resolution?: number;
          /**
           * @description Selected model.
           * @default stablediffusion-xl
           * @enum {string}
           */
          model?: "stablediffusion-xl";
          /**
           * Format: float
           * @description Controls the strength of the generation process.
           * @default 8
           */
          strength?: number;
          /**
           * Format: float
           * @description Controls the influence of the image prompt on the generated output.
           * @default 5
           */
          image_prompt_influence?: number;
          /** @description Negative input prompt. */
          negative_prompt?: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
          /** @description Seed for deterministic generation. Default is a random seed. */
          seed?: number;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
            /** @description The random noise seed used for generation. */
            seed: number;
          };
        };
      };
    };
  };
  /**
   * MultiGenerativeEditImage
   * @description Generate multiple image outputs modifying part of an image using a mask.
   */
  MultiGenerativeEditImage: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Original image. */
          image_uri: string;
          /** @description Text prompt. */
          prompt: string;
          /** @description Mask image that controls which pixels are edited (inpainting). If unset, the entire image is edited (image-to-image). */
          mask_image_uri?: string;
          /** @description Image prompt. */
          image_prompt_uri?: string;
          /** @description Number of images to generate. */
          num_images: number;
          /**
           * @description Resolution of the output image, in pixels.
           * @default 1024
           */
          output_resolution?: number;
          /**
           * @description Selected model.
           * @default stablediffusion-xl
           * @enum {string}
           */
          model?: "stablediffusion-xl";
          /** @description Negative input prompt. */
          negative_prompt?: string;
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
          /**
           * Format: float
           * @description Controls the strength of the generation process.
           * @default 8
           */
          strength?: number;
          /**
           * Format: float
           * @description Controls the influence of the image prompt on the generated output.
           * @default 5
           */
          image_prompt_influence?: number;
          /** @description Random noise seeds. Default is random seeds for each generation. */
          seeds?: number[];
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            outputs: {
                /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
                image_uri: string;
                /** @description The random noise seed used for generation. */
                seed: number;
              }[];
          };
        };
      };
    };
  };
  /**
   * FillMask
   * @description Edit an image with a generative model.
   */
  FillMask: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Input image. */
          image_uri: string;
          /** @description Mask image that controls which pixels are inpainted. */
          mask_image_uri: string;
          /**
           * @description Selected model.
           * @default big-lama
           * @enum {string}
           */
          model?: "big-lama";
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
          };
        };
      };
    };
  };
  /**
   * UpscaleImage
   * @description Upscale an image.
   */
  UpscaleImage: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Input image. */
          image_uri: string;
          /**
           * @description Selected model.
           * @default real-esrgan-x4
           * @enum {string}
           */
          model?: "real-esrgan-x4";
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
          };
        };
      };
    };
  };
  /**
   * RemoveBackground
   * @description Remove the background from an image, with the option to return the foreground as a mask.
   */
  RemoveBackground: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Input image. */
          image_uri: string;
          /** @description Return a mask image instead of the original content. */
          return_mask?: boolean;
          /** @description Hex value background color. Transparent if unset. */
          background_color?: string;
          /**
           * @description Selected model.
           * @default isnet
           * @enum {string}
           */
          model?: "isnet";
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            image_uri: string;
          };
        };
      };
    };
  };
  /**
   * DetectSegments
   * @description Detect segments in an image given point(s) or bounding box(es).
   */
  DetectSegments: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Input image. */
          image_uri: string;
          /** @description Point prompts, to detect a segment under the point. One of `point_prompt` or `box_prompt` must be set. */
          point_prompts?: {
              /** @description X position. */
              x: number;
              /** @description Y position. */
              y: number;
            }[];
          /** @description Box prompts, to detect a segment within the bounding box. One of `point_prompt` or `box_prompt` must be set. */
          box_prompts?: {
              /**
               * Format: float
               * @description Top left corner x.
               */
              x1: number;
              /**
               * Format: float
               * @description Top left corner y.
               */
              y1: number;
              /**
               * Format: float
               * @description Bottom right corner x.
               */
              x2: number;
              /**
               * Format: float
               * @description Bottom right corner y.
               */
              y2: number;
            }[];
          /**
           * @description Selected model.
           * @default segment-anything
           * @enum {string}
           */
          model?: "segment-anything";
          /** @description Use "hosted" to return an image URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the image data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Detected segments in 'mask image' format. Base 64-encoded JPEG image bytes, or a hosted image url if `store` is provided. */
            mask_image_uri: string;
          };
        };
      };
    };
  };
  /**
   * TranscribeMedia
   * @description Transcribe speech in an audio or video file.
   */
  TranscribeMedia: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Input audio. */
          audio_uri: string;
          /** @description Prompt to guide model on the content and context of input audio. */
          prompt?: string;
          /**
           * @description Language of input audio in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format.
           * @default en
           */
          language?: string;
          /**
           * @description Segment the text into sentences with approximate timestamps.
           * @default false
           */
          segment?: boolean;
          /**
           * @description Align transcription to produce more accurate sentence-level timestamps and word-level timestamps. An array of word segments will be included in each sentence segment.
           * @default false
           */
          align?: boolean;
          /**
           * @description Identify speakers for each segment. Speaker IDs will be included in each segment.
           * @default false
           */
          diarize?: boolean;
          /**
           * @description Suggest automatic chapter markers.
           * @default false
           */
          suggest_chapters?: boolean;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Transcribed text. */
            text: string;
            /** @description Transcribed segments, if `segment` is enabled. */
            segments?: {
                /** @description Text of segment. */
                text: string;
                /**
                 * Format: float
                 * @description Start time of segment, in seconds.
                 */
                start: number;
                /**
                 * Format: float
                 * @description End time of segment, in seconds.
                 */
                end: number;
                /** @description ID of speaker, if `diarize` is enabled. */
                speaker?: string;
                /** @description Aligned words, if `align` is enabled. */
                words?: {
                    /** @description Text of word. */
                    word: string;
                    /**
                     * Format: float
                     * @description Start time of word, in seconds.
                     */
                    start?: number;
                    /**
                     * Format: float
                     * @description End time of word, in seconds.
                     */
                    end?: number;
                    /** @description ID of speaker, if `diarize` is enabled. */
                    speaker?: string;
                  }[];
              }[];
            /** @description Chapter markers, if `suggest_chapters` is enabled. */
            chapters?: {
                /** @description Chapter title. */
                title: string;
                /**
                 * Format: float
                 * @description Start time of chapter, in seconds.
                 */
                start: number;
              }[];
          };
        };
      };
    };
  };
  /**
   * GenerateSpeech
   * @description Generate speech from text.
   */
  GenerateSpeech: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Input text. */
          text: string;
          /** @description Reference audio used to synthesize the speaker. If unset, a default speaker voice will be used. */
          audio_uri?: string;
          /**
           * @description Language of input text. Supported languages: `en, de, fr, es, it, pt, pl, zh, ar, cs, ru, nl, tr, hu, ko`.
           * @default en
           */
          language?: string;
          /** @description Use "hosted" to return an audio URL hosted on Substrate. You can also provide a URL to a registered [file store](/docs/file-stores). If unset, the audio data will be returned as a base64-encoded string. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Base 64-encoded WAV audio bytes, or a hosted audio url if `store` is provided. */
            audio_uri: string;
          };
        };
      };
    };
  };
  /**
   * EmbedText
   * @description Generate vector embedding for a text document.
   */
  EmbedText: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Text to embed. */
          text: string;
          /**
           * @description Selected model.
           * @default jina-v2
           * @enum {string}
           */
          model?: "jina-v2" | "clip";
          /** @description [Vector store](/docs/vector-stores) identifier. */
          store?: string;
          /** @description Metadata that can be used to query the vector store. Ignored if `store` is unset. */
          metadata?: Record<string, never>;
          embedded_metadata?: {
            /** @description Keys to embed with text. */
            include_keys?: string[];
            /** @description Keys to exclude. All other keys will be embedded with text. */
            exclude_keys?: string[];
          };
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** Embedding */
            embedding: {
              /** @description Embedding vector. */
              vector: string;
            };
          };
        };
      };
    };
  };
  /**
   * MultiEmbedText
   * @description Generate vector embeddings for multiple text documents.
   */
  MultiEmbedText: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Items to embed. */
          items: {
              /** @description Text to embed. */
              text: string;
              /** @description Metadata that can be used to query the vector store. Ignored if `store` is unset. */
              metadata?: Record<string, never>;
            }[];
          /**
           * @description Selected model.
           * @default jina-v2
           * @enum {string}
           */
          model?: "jina-v2" | "clip";
          /** @description [Vector store](/docs/vector-stores) identifier. */
          store?: string;
          embedded_metadata?: {
            /** @description Keys to embed with text. */
            include_keys?: string[];
            /** @description Keys to exclude. All other keys will be embedded with text. */
            exclude_keys?: string[];
          };
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated embeddings. */
            embeddings: {
                /** @description Embedding vector. */
                vector: string;
              }[];
          };
        };
      };
    };
  };
  /**
   * EmbedImage
   * @description Generate vector embedding for an image, and optionally store the embedding.
   */
  EmbedImage: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Image to embed. */
          image_uri: string;
          /**
           * @description Selected model.
           * @default clip
           * @enum {string}
           */
          model?: "clip";
          /** @description [Vector store](/docs/vector-stores) identifier. */
          store?: string;
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** Embedding */
            embedding: {
              /** @description Embedding vector. */
              vector: string;
            };
          };
        };
      };
    };
  };
  /**
   * MultiEmbedImage
   * @description Generate vector embeddings for multiple images, and optionally store the embeddings.
   */
  MultiEmbedImage: {
    parameters: {
      query?: {
        undefined?: {
          /** @description Items to embed. */
          items: {
              /** @description Image to embed. */
              image_uri: string;
            }[];
          /** @description [Vector store](/docs/vector-stores) identifier. */
          store?: string;
          /**
           * @description Selected model.
           * @default clip
           * @enum {string}
           */
          model?: "clip";
        };
      };
    };
    responses: {
      /** @description OK */
      200: {
        content: {
          "application/json": {
            /** @description Generated embeddings. */
            embeddings: {
                /** @description Embedding vector. */
                vector: string;
              }[];
          };
        };
      };
    };
  };
  /**
   * /vector-stores/create
   * @description Create a vector store for storing and querying embeddings.
   */
  "/vector-stores/create": {
    parameters: {
      query?: {
        undefined?: {
          /** @description Vector store name. */
          name: string;
          /**
           * @description Selected embedding model
           * @enum {string}
           */
          model: "jina-v2" | "clip";
          /**
           * @description The max number of connections per layer for the index.
           * @default 16
           */
          m?: number;
          /**
           * @description The size of the dynamic candidate list for constructing the index graph.
           * @default 64
           */
          ef_construction?: number;
          /**
           * @description The distance metric to construct the index with.
           * @default inner
           * @enum {string}
           */
          metric?: "cosine" | "l2" | "inner";
        };
      };
    };
    responses: {
      /** @description Vector store created. */
      200: {
        content: {
          "application/json": {
            /** @description Vector store name. */
            name: string;
            /**
             * @description Selected embedding model
             * @enum {string}
             */
            model: "jina-v2" | "clip";
            /**
             * @description The max number of connections per layer for the index.
             * @default 16
             */
            m?: number;
            /**
             * @description The size of the dynamic candidate list for constructing the index graph.
             * @default 64
             */
            ef_construction?: number;
            /**
             * @description The distance metric to construct the index with.
             * @default inner
             * @enum {string}
             */
            metric?: "cosine" | "l2" | "inner";
          };
        };
      };
    };
  };
  /**
   * /vector-stores/list
   * @description List all vector stores.
   */
  "/vector-stores/list": {
    responses: {
      /** @description List of vector stores. */
      200: {
        content: {
          "application/json": ({
              /** @description Vector store name. */
              name: string;
              /**
               * @description Selected embedding model
               * @enum {string}
               */
              model: "jina-v2" | "clip";
              /**
               * @description The max number of connections per layer for the index.
               * @default 16
               */
              m?: number;
              /**
               * @description The size of the dynamic candidate list for constructing the index graph.
               * @default 64
               */
              ef_construction?: number;
              /**
               * @description The distance metric to construct the index with.
               * @default inner
               * @enum {string}
               */
              metric?: "cosine" | "l2" | "inner";
            })[];
        };
      };
    };
  };
  /**
   * /vector-stores/delete
   * @description Delete a vector store.
   */
  "/vector-stores/delete": {
    parameters: {
      query?: {
        undefined?: {
          /** @description Vector store name. */
          name: string;
          /**
           * @description Selected embedding model
           * @enum {string}
           */
          model: "jina-v2" | "clip";
        };
      };
    };
    responses: {
      /** @description Vector store deleted. */
      204: {
        content: never;
      };
    };
  };
  /**
   * /vector-stores/query
   * @description Query a vector store for similar vectors.
   */
  "/vector-stores/query": {
    parameters: {
      query?: {
        undefined?: {
          /** @description Vector store to query against. */
          name: string;
          /**
           * @description Selected embedding model
           * @enum {string}
           */
          model: "jina-v2" | "clip";
          /** @description Document IDs to use for the query. */
          query_ids?: string[];
          /** @description Image URIs to embed and use for the query. */
          query_image_uris?: string[];
          /** @description Vector to use for the query. */
          query_vectors?: number[][];
          /** @description Texts to embed and use for the query. */
          query_strings?: string[];
          /**
           * @description Number of results to return.
           * @default 10
           */
          top_k?: number;
          /**
           * @description The size of the dynamic candidate list for searching the index graph.
           * @default 40
           */
          ef_search?: number;
          /**
           * @description Include the values of the vectors in the response.
           * @default false
           */
          include_values?: boolean;
          /**
           * @description Include the metadata of the vectors in the response.
           * @default false
           */
          include_metadata?: boolean;
          /** @description Filter metadata by key-value pairs. */
          filters?: Record<string, never>;
        };
      };
    };
    responses: {
      /** @description Query results. */
      200: {
        content: {
          "application/json": {
            /** @description Query results. */
            results: {
                  /** @description Document ID. */
                  id: string;
                  /**
                   * Format: float
                   * @description Similarity score.
                   */
                  distance: number;
                  /** @description Embedding vector. */
                  vector?: number[];
                  /** @description Document metadata. */
                  metadata?: Record<string, never>;
                }[][];
            /** @description Vector store name. */
            name?: string;
            /**
             * @description Selected embedding model
             * @enum {string}
             */
            model?: "jina-v2" | "clip";
            /**
             * @description The distance metric used for the query.
             * @enum {string}
             */
            metric?: "cosine" | "l2" | "inner";
          };
        };
      };
    };
  };
  /**
   * /vectors/fetch
   * @description Fetch vectors from a vector store.
   */
  "/vectors/fetch": {
    parameters: {
      query?: {
        undefined?: {
          /** @description Vector store name. */
          name: string;
          /**
           * @description Selected embedding model
           * @enum {string}
           */
          model: "jina-v2" | "clip";
          /** @description Document IDs to retrieve. */
          ids: string[];
        };
      };
    };
    responses: {
      /** @description Vector data. */
      200: {
        content: {
          "application/json": {
            /** @description Retrieved vectors. */
            vectors: {
                /** @description Document ID. */
                id: string;
                /** @description Embedding vector. */
                vector: number[];
                /** @description Document metadata. */
                metadata: Record<string, never>;
              }[];
          };
        };
      };
    };
  };
  /**
   * /vectors/update
   * @description Update vectors in a vector store.
   */
  "/vectors/update": {
    parameters: {
      query?: {
        undefined?: {
          /** @description Vector store name. */
          name: string;
          /**
           * @description Selected embedding model
           * @enum {string}
           */
          model: "jina-v2" | "clip";
          /** @description Vectors to upsert. */
          vectors: {
              /** @description Document ID. */
              id: string;
              /** @description Embedding vector. */
              vector?: number[];
              /** @description Document metadata. */
              metadata?: Record<string, never>;
            }[];
        };
      };
    };
    responses: {
      /** @description Count of updated vectors. */
      200: {
        content: {
          "application/json": {
            /** @description Number of vectors modified. */
            count: number;
          };
        };
      };
    };
  };
  /**
   * /vectors/delete
   * @description Delete vectors in a vector store.
   */
  "/vectors/delete": {
    parameters: {
      query?: {
        undefined?: {
          /** @description Vector store name. */
          name: string;
          /**
           * @description Selected embedding model
           * @enum {string}
           */
          model: "jina-v2" | "clip";
          /** @description Document IDs to delete. */
          ids: string[];
        };
      };
    };
    responses: {
      /** @description Count of deleted vectors. */
      200: {
        content: {
          "application/json": {
            /** @description Number of vectors modified. */
            count: number;
          };
        };
      };
    };
  };
}
